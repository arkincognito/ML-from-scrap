{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 딥러닝 온라인 심화반 <Level 6>에 오신 것을 환영합니다!\n",
    "\n",
    "<Level 6>의 주제는 <U>Multi Layer Perceptron(MLP, 다층 퍼셉트론)</U>입니다. 우리는 퍼셉트론이 가지고 있는 가장 큰 문제들 중 하나인 XOR 문제에 대해 알아보고, MLP의 개념과 예측 및 학습 방법에 대해 배웠습니다. 마지막으로 우편번호 손글씨 이미지(MNIST)를 인식하는 알고리즘을 업그레이드 해보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "이전과 같이 MNIST 문제를 풀기 위한 환경 설정을 해줍니다. MNIST 필기체 데이터셋이 내장되어 있는 **keras** 라이브러리가 설치되어 있는지 확인해주세요. 또한 **keras**는 **tensorflow** 상에서 구현된 라이브러리이므로(고수준 API라고 함) **tensorflow**가 우선적으로 설치되어 있어야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore') # 경고 메시지를 숨길 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파이썬을 아나콘다로 설치한 경우, 아래 명령어 대신 아나콘다 네비게이터(Anaconda Navigator)를 실행한 뒤\n",
    "# 좌측의 환경(Environment) 탭에서 설치할 수 있습니다.\n",
    "# !pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파이썬을 아나콘다로 설치한 경우, 아래 명령어 대신 아나콘다 네비게이터(Anaconda Navigator)를 실행한 뒤\n",
    "# 좌측의 환경(Environment) 탭에서 설치할 수 있습니다.\n",
    "# !pip install keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST Dataset\n",
    "\n",
    "이번 과제는 [MNIST 필기체 데이터셋](http://yann.lecun.com/exdb/mnist/)을 활용하여 필기체 이미지를 인식하는 이미지 인식 알고리즘을 **Multi Layer Perceptron**로 작성합니다. 복습 차원에서 퍼셉트론 또는 Single Layer Neural Network을 이용해 MNIST 문제를 해결한 다음, Multi Layer Perceptron으로 이것을 개선해보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. MNIST 데이터셋 구성**\n",
    "> 가로 28px, 세로 28px의 필기체 이미지가 주어지며, 필기체는 숫자 0부터 9까지 총 10개의 Label로 구성되어 있습니다. 이미지는 컬러가 없는 흑백 데이터이며, 한 픽셀의 값은 0 ~ 255입니다. (0일수록 어둡고, 255일수록 밝습니다.)\n",
    "\n",
    "**2. Train, Test Set**\n",
    "> 데이터는 60,000개의 Train 데이터와 10,000개의 Test 데이터가 주어지는데, Train 데이터로 Multi-layer Neural Network를 학습한 뒤 Test 데이터로 정확도(accuracy)를 측정합니다. 각 변수의 세부 정보는 다음과 같습니다.\n",
    "\n",
    "   * **X_train**: **Train 데이터의 Feature**입니다. 가로 28px, 세로 28px, 총 60,000개의 데이터로 구성되어 있습니다. 픽셀 하나의 값은 0 ~ 255입니다.\n",
    "   * **y_train**: **Train 데이터의 Label**입니다. 총 60,000개이며, 이미지가 어떤 숫자를 나타내는지가 적혀 있습니다. 값은 0부터 9까지 입니다.  \n",
    "   * **X_test**: **Test 데이터의 Feature**입니다. 가로 28px, 세로 28px, 총 10,000개의 데이터로 구성되어 있습니다. 픽셀 하나의 값은 0 ~ 255입니다.\n",
    "   * **y_test**: **Test 데이터의 Label** 입니다. 총 10,000 개이며, 이미지가 어떤 숫자를 나타내는지가 적혀 있습니다. 값은 0부터 9까지입니다.\n",
    "\n",
    "\n",
    "**3. 주의 사항**\n",
    "\n",
    "  * 이전에 ***XOR data를 풀었던 코드를 조금만 응용***하면 매우 쉽게 MNIST 데이터셋 문제를 풀 수 있습니다.\n",
    "  * Accuracy가 잘 올라가지 않고 그 이유를 잘 모르겠다면, ***Loss Function(=Cross Entropy)를 병행***해서 사용해보세요. 앞서 언급드린대로 Loss Function은 <U>학습이 잘 될수록 0에 수렴하고, 학습이 잘 되지 않을수록 무한대로 발산합니다.</U> 즉, Loss Function을 사용할 결과가 무한대로 발산하고 있다면 무언가 제대로 풀리지 않고 있다는 것입니다.\n",
    "  * Loss가 제대로 떨어지지 않는다면 문제는 크게 두 가지입니다. 1) weight의 초기값이 좋지 않거나, 2) learning rate가 너무 높거나 낮아서 생기는 문제입니다. Loss가 떨어지지 않는다면 ***weight의 초기값과 learning rate를 바꿔보세요.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) (10000, 28, 28)\n",
      "(60000,) (10000,)\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "\n",
    "# 케라스 라이브러리에서 MNIST 데이터셋을 불러옵니다.\n",
    "# 다소 시간이 걸립니다.\n",
    "((X_train, y_train), (X_test, y_test)) = mnist.load_data()\n",
    "\n",
    "print(X_train.shape, X_test.shape)\n",
    "print(y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 0 4 1 9 2 1 3 1 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x140c11048>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABBIAAAG9CAYAAACoKlVaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3XmUXWWZL/7nZZRZIIgRlSCigjQEDDYgC+hmUGlk0MvUjLZNvCqCvRouiFylRRRtoRulRaNAQOlGbiODXm3gAgooshguaBgU8DLEREABiYAEyPv7I4efEd5dtc+uM6TO/nzWyqqq59TZz7sr9c1Jnuzab8o5BwAAAEAdywx7AQAAAMDkYZAAAAAA1GaQAAAAANRmkAAAAADUZpAAAAAA1GaQAAAAANQ2lEFCSuldKaVfpJTuTSkdN+De96eUfp5Sui2ldHOfe52dUnokpTRnidpaKaUrU0r3dN6uOcDeJ6aUft0599tSSrv1oe/rUkrXpJTuSindkVI6qlPv+3mP0bvv5z0obclOp99Q8jOs7HT6yE8ftSU/bXzt6fSRnz6SH/npx7m3ITsR8jPK+Wn1a0/OeaC/ImLZiLgvIt4QEStExO0RsckA+98fEVMG1Gv7iNgyIuYsUftCRBzXef+4iPj8AHufGBFH9/mcp0bElp33V4uIX0bEJoM47zF69/28B/T91JrsdPoNJT/Dyk6nj/z072vbmvy08bWn00d++ve1lR/56cu5j3p2OuclPyOcnza/9gzjioS3R8S9Oedf5ZwXRsQFEbHnENbRdznnayPisZeU94yIczvvnxsRew2wd9/lnOfnnG/tvL8gIu6KiPViAOc9Ru9R0ZrsRAwvP8PKTqe3/PRPa/LTxteeTm/56R/5kZ+IPpx7C7ITIT8jnZ82v/YMY5CwXkQ8tMTHc2Owf2DkiLgipXRLSmnmAPu+aN2c8/yIxb/5EfGqAfc/IqX0s87lP325tOhFKaVpEbFFRNwYAz7vl/SOGOB591HbsxMx3PwM9HtIfnqu7flpzWtPhPz0gfzIT9/PfUSzEyE/rclP2157hjFISIVaHmD/d+Sct4yId0fER1JK2w+w97CdGREbRsT0iJgfEaf2q1FKadWIuCgiPpZzfrJffWr2Hth595nsDM9Av4fkpy/kZ3jkR34mSn5GPD8jnJ0I+Rkm//bp43kPY5AwNyJet8THr42IeYNqnnOe13n7SERcHIsvNxqkh1NKUyMiOm8fGVTjnPPDOecXcs6LIuLr0adzTyktH4u/mc/POX+nUx7IeZd6D+q8B6Dt2YkYUn4G+T0kP33T9vyM/GtPhPz0kfzIT9/OfcSzEyE/I5+ftr72DGOQcFNEbJRS2iCltEJE7B8Rlw2icUpplZTSai++HxG7RsScsZ/Vc5dFxKGd9w+NiEsH1fjFb+aOvaMP555SShFxVkTclXM+bYmH+n7eVb0Hcd4D0vbsRAwpP4P6HpKfvmp7fkb6tafTR376R37kJ6IP596C7ETIz0jnp9WvPXkAd/B86a+I2C0W31Xyvoj4xAD7viEW3yn19oi4o9+9I+I/YvHlJM/F4mnkByJi7Yi4KiLu6bxda4C9vxkRP4+In8Xib+6pfei7XSy+XOtnEXFb59dugzjvMXr3/bwH9ast2en0HEp+hpWdTm/56e/3VCvy08bXnk5v+env95X8yE/Pz70N2emcp/yMaH7a/NqTOosAAAAAGNcwfrQBAAAAmKQMEgAAAIDaDBIAAACA2gwSAAAAgNoMEgAAAIDahjZISCnN1FvvUe/dL239eurdjr791sbfy2H2buM5D7t3v7T166l3u3r3S1u/nnqPbu9hXpEwzD8g9NZ7smvr11PvdvTttzb+Xg6zdxvPedi9+6WtX0+929W7X9r69dR7RHtPaJCQUnpXSukXKaV7U0rH9WpR0AbyA83JDzQnP9Cc/MBiKefc7IkpLRsRv4yIXSJibkTcFBEH5JzvHOM5zZrBEOWcU6+PKT+0hfxAc0tDfmSHSeq3Oed1en1Q+aEN6r72TOSKhLdHxL0551/lnBdGxAURsecEjgdtIj/QnPxAc/JDGzzQp+PKD3RMZJCwXkQ8tMTHczs1YHzyA83JDzQnP9Cc/EDHchN4bumSh5ddvtO5a+Qo3jAFJkJ+oDn5gebGzY/sQCX5gY6JDBLmRsTrlvj4tREx76WflHOeFRGzIvycECxBfqA5+YHmxs2P7EAl+YGOifxow00RsVFKaYOU0goRsX9EXNabZcHIkx9oTn6gOfmB5uQHOhpfkZBzfj6ldEREXB4Ry0bE2TnnO3q2Mhhh8gPNyQ80Jz/QnPzAnzTe/rFRM5f3MAn1Y/utJuSHyUh+oLmlIT+ywyR1S855xrAXIT9MRoPY/hEAAABoGYMEAAAAoDaDBAAAAKA2gwQAAACgNoMEAAAAoDaDBAAAAKA2gwQAAACgNoMEAAAAoDaDBAAAAKA2gwQAAACgNoMEAAAAoDaDBAAAAKA2gwQAAACgNoMEAAAAoDaDBAAAAKA2gwQAAACgtuWGvQCApdXb3va2Yv2II46ofM4hhxxSrJ933nnF+pe//OVi/dZbbx1ndQAAMByuSAAAAABqM0gAAAAAajNIAAAAAGozSAAAAABqM0gAAAAAaks55+ZPTun+iFgQES9ExPM55xnjfH7zZi2z7LLLFutrrLFGT44/1l3nV1555WL9zW9+c7H+kY98pFj/4he/WKwfcMABlb3/+Mc/FuunnHJKsf5P//RPlcfqlZxz6sdx5WfpMX369GL96quvLtZXX331nvX+/e9/X6yvvfbaPesxTPLDMOy0007F+vnnn1/5nB122KFY/8UvftGTNTWxNORHdia3E044oViv+vvTMsuU/49xxx13rOzxox/9qOt1DcAt470uNCU/jLq6rz292P7xr3LOv+3BcaCN5Aeakx9oTn6gOfmh9fxoAwAAAFDbRAcJOSKuSCndklKa2YsFQYvIDzQnP9Cc/EBz8gMx8R9teEfOeV5K6VURcWVK6e6c87VLfkInYEIGLyc/0Jz8QHNj5kd2YEzyAzHBKxJyzvM6bx+JiIsj4u2Fz5mVc57RrxuewGQlP9Cc/EBz4+VHdqCa/MBija9ISCmtEhHL5JwXdN7fNSI+3bOVLcVe//rXF+srrLBCsb7tttsW69ttt11lj1e+8pXF+vve975xVtc/c+fOLda/9KUvFet77713sb5gwYLKHrfffnuxvpTeEbixNudnmN7+9pf9WzMiIi666KJivWqXlLF2u6n6/l64cGGxXrU7w9Zbb12s33rrrZW9q3qMmqU1P9tvv32xXvV7fPHFF/dzOa221VZbFes33XTTgFey9Fla88PEHHbYYcX6scceW6wvWrSoq+NPZJe3USI/8CcT+dGGdSPi4pTSi8f595zzf/VkVTD65Aeakx9oTn6gOfmBjsaDhJzzryJi8x6uBVpDfqA5+YHm5Aeakx/4E9s/AgAAALUZJAAAAAC1GSQAAAAAtU3kZosjbfr06ZWPXX311cV61R3eJ5uqO/mecMIJxfof/vCHYv38888v1ufPn1/Z+/HHHy/Wf/GLX1Q+h3ZaeeWVKx/bcssti/VvfetbxfrUqVN7sqaIiHvuuadY/8IXvlCsX3DBBcX6j3/842K9KocREZ/73OfGWR39tOOOOxbrG220UbFu14aJW2aZ8v+HbLDBBsX6+uuvX3mszs3TYFKq+t5+xSteMeCVQDN/+Zd/WawfdNBBxfoOO+xQrL/1rW/tuvfRRx9drM+bN69Yr9p5r+rvmTfeeGPXa5oMXJEAAAAA1GaQAAAAANRmkAAAAADUZpAAAAAA1GaQAAAAANRmkAAAAADUZvvHCg8++GDlY7/73e+K9WFu/1i1rcgTTzxRrP/VX/1V5bEWLlxYrH/zm9/sfmHQJ1/72tcqHzvggAMGuJI/V7X15Kqrrlqs/+hHPyrWq7YS3GyzzRqti/475JBDivUbbrhhwCtpj6qtWw8//PBivWprroiIu+++uydrgn7ZeeedKx/76Ec/2tWxqr7fd99992L94Ycf7ur4ULLffvtVPnb66acX61OmTCnWq7bs/eEPf1isr7POOpW9//mf/7nysW56V/XYf//9uzr+ZOGKBAAAAKA2gwQAAACgNoMEAAAAoDaDBAAAAKA2gwQAAACgNrs2VHjssccqHzvmmGOK9ao73f7f//t/i/UvfelLXa/rtttuK9Z32WWXYv2pp54q1t/61rdW9jjqqKO6Xhf0y9ve9rZi/W/+5m8qn1N1N90qVTsnfPe73y3Wv/jFL1Yea968ecV61Z8Djz/+eLH+13/918V6t+fG4CyzjNn8oH3jG9/o6vPvueeePq0Eeme77bYr1s8555zK53S7c1jVXeofeOCBro5Duy23XPmfkjNmzCjWv/71r1cea+WVVy7Wr7322mL9pJNOKtavv/76Yn3FFVes7H3hhRcW67vuumvlc0puvvnmrj5/svO3HgAAAKA2gwQAAACgNoMEAAAAoDaDBAAAAKA2gwQAAACgtnF3bUgpnR0Ru0fEIznnTTu1tSLi2xExLSLuj4h9c87lW4+PoEsuuaRYv/rqq4v1BQsWFOubb755ZY8PfOADxXrV3eKrdmeocscdd1Q+NnPmzK6ORTX5qW/69OnF+pVXXlmsr7766pXHyjkX6z/4wQ+K9QMOOKBY32GHHYr1E044obJ31V3kH3300WL99ttvL9YXLVpUrI+1W8WWW25ZrN96662Vz1maLa352WyzzYr1ddddd5DLILq/U33VnyejaGnND+M79NBDi/XXvOY1XR/rhz/8YbF+3nnndX2sNpGfeg466KBivdsddSKq/3zeb7/9ivUnn3yyq+NXHSei+90Z5s6dW6yfe+65XR1nsqtzRcLsiHjXS2rHRcRVOeeNIuKqzsfAy80O+YGmZof8QFOzQ36gqdkhPzCmcQcJOedrI+Kxl5T3jIgXRy7nRsRePV4XjAT5gebkB5qTH2hOfmB8Te+RsG7OeX5EROftq3q3JBh58gPNyQ80Jz/QnPzAEsa9R8JEpZRmRoQfuocG5Aeakx9oRnagOfmhLZpekfBwSmlqRETn7SNVn5hznpVznpFzntGwF4wa+YHm5Aeaq5Uf2YEi+YElNL0i4bKIODQiTum8vbRnK5rEur176O9///uuexx++OHF+re//e1iverO7wxVq/Pzpje9qVg/5phjivWqu7L/9re/rewxf/78Yr3qbrp/+MMfivX//b//d1f1QVhppZUqH/vHf/zHYv3AAw/s13KGYej52W233Yr1sX5vmJiqHTE22GCDro7z61//uhfLmcyGnh/+ZMqUKcX63/3d3xXrY/2d7oknnijWP/OZz3S/MKq0Nj8nnXRSsX788ccX61W7Z33lK1+p7FG1I1a3/76q8olPfKInx4mIOPLII4v1qh26RtW4VySklP4jIm6IiDenlOamlD4QiwO0S0rpnojYpfMx8BLyA83JDzQnP9Cc/MD4xr0iIedc3mA9YqcerwVGjvxAc/IDzckPNCc/ML6m90gAAAAAWsggAQAAAKjNIAEAAACozSABAAAAqK3p9o/0wIknnlj52Nve9rZifYcddijWd95552L9iiuu6HpdMFErrrhi5WNf/OIXi/WqLfUWLFhQrB9yyCGVPW6++eZifdS353v9618/7CW0wpvf/OauPv+OO+7o00rao+rPjaptIX/5y18W61V/nkA/TZs2rVi/6KKLetbjy1/+crF+zTXX9KwHo+2Tn/xk5WNV2zwuXLiwWL/88suL9WOPPbayxzPPPDPG6l7uFa94RbG+6667Futj/R0ppVSsV22feumlrdn5c0yuSAAAAABqM0gAAAAAajNIAAAAAGozSAAAAABqM0gAAAAAarNrwxA99dRTlY8dfvjhxfqtt95arH/9618v1qvu1lt1V/uIiH/7t38r1nPOlc+BJW2xxRaVj1XtzlBlzz33LNZ/9KMfdXUcGJabbrpp2EsYitVXX71Yf9e73lX5nIMOOqhYr7oLd5WTTjqpWH/iiSe6Og70QtX3/GabbdbVca666qrKx04//fSujkV7vfKVryzWP/zhD1c+p+rfAFW7M+y1117dL6zCG9/4xmL9/PPPL9ardr4by3/+538W61/4whe6PlabuCIBAAAAqM0gAQAAAKjNIAEAAACozSABAAAAqM0gAQAAAKjNrg1Lqfvuu69YP+yww4r1c845p1g/+OCDu6pHRKyyyirF+nnnnVesz58/v/JYtNNpp51W+VhKqViv2oWhrbszLLNMec67aNGiAa+EiVprrbX63mPzzTcv1qvytvPOO1ce67WvfW2xvsIKKxTrBx54YLFe9T38zDPPVPa+8cYbi/Vnn322WF9uufJfY2655ZbKHtAvVXeqP+WUU7o6zvXXX1+sH3rooZXP+f3vf99VD9qr6s/yKVOmdH2sI488slh/1ateVay///3vrzzWHnvsUaxvuummxfqqq65arFftMDHW7nPf+ta3ivWxdtjDFQkAAABAFwwSAAAAgNoMEgAAAIDaDBIAAACA2gwSAAAAgNrG3bUhpXR2ROweEY/knDft1E6MiMMj4tHOpx2fc/5+vxbJn1x88cXF+j333FOsV909f6eddqrs8dnPfrZYX3/99Yv1k08+uVj/9a9/XdmjLUY9P7vvvnuxPn369MrnVN0197LLLuvJmkZF1e4MY911+LbbbuvXcoZiac1P1a4DVb83X/3qV4v1448/vmdr2myzzYr1ql0bnn/++cpjPf3008X6nXfeWayfffbZxfrNN99crI+1E8vDDz9crM+dO7dYX2mllYr1u+++u7JHWyyt+Znspk2bVvnYRRdd1JMev/rVr4r1qnzQe6Ocn4ULFxbrjz76aLEeEbHOOusU6//v//2/Yn2sv6t0a968ecX6k08+WaxPnTq1WP/tb39b2eO73/1u9wuj1hUJsyPiXYX6v+Scp3d+TboQwYDMDvmBpmaH/EBTs0N+oKnZIT8wpnEHCTnnayPisQGsBUaO/EBz8gPNyQ80Jz8wvoncI+GIlNLPUkpnp5TW7NmKoB3kB5qTH2hOfqA5+YGOpoOEMyNiw4iYHhHzI+LUqk9MKc1MKd2cUir/sCS0j/xAc/IDzdXKj+xAkfzAEhoNEnLOD+ecX8g5L4qIr0fE28f43Fk55xk55xlNFwmjRH6gOfmB5urmR3bg5eQH/ty4uzaUpJSm5pzndz7cOyLm9G5JNDFnTvm3YN999y3W3/Oe91Qe65xzzinWP/jBDxbrG220UbG+yy67VPZos1HKT9Ud01dYYYXK5zzyyCPF+re//e2erGlpteKKKxbrJ554YlfHufrqqysf+/jHP97VsSajpSE/H/7wh4v1Bx54oFjfdttt+7mciIh48MEHi/VLLrmkWL/rrrsqj/XTn/60J2tqYubMmcV61R3Dq+5uT9nSkJ/J7thjj618rGq3nW6dcsopPTkOvTUq+XniiSeK9b322qvyOd/73veK9bXWWqtYv++++4r1Sy+9tLLH7Nmzi/XHHivfquKCCy4o1qt2baj6fJqrs/3jf0TEjhExJaU0NyI+FRE7ppSmR0SOiPsjovwvTGg5+YHm5Aeakx9oTn5gfOMOEnLOBxTKZ/VhLTBy5Aeakx9oTn6gOfmB8U1k1wYAAACgZQwSAAAAgNoMEgAAAIDaDBIAAACA2hpt/8jkUbXFyze/+c3K53zjG98o1pdbrvztsv322xfrO+64Y7H+wx/+sLI3o+/ZZ58t1ufPn1+sTzZV2zyecMIJxfoxxxxTrM+dO7dYP/XUUyt7/+EPfxhndfTT5z//+WEvYdLbaaeduvr8iy66qE8roe2mT59erO+6664961G1Fd4vfvGLnvWAum688cbKx6q24B2Eqn9n7LDDDsV61TastgvuPVckAAAAALUZJAAAAAC1GSQAAAAAtRkkAAAAALUZJAAAAAC12bVhRGy22WbF+n/7b/+tWN9qq60qj1W1O0OVO++8s1i/9tpruzoO7XDZZZcNewkTVnU374jqXRj222+/Yr3qrt3ve9/7ul8YtMzFF1887CUwoq644opifc011+z6WD/96U+L9cMOO6zrY0HbrLTSSsV61e4MOedi/YILLujZmljMFQkAAABAbQYJAAAAQG0GCQAAAEBtBgkAAABAbQYJAAAAQG12bVhKvfnNby7WjzjiiGL9ve99b7H+6le/umdreuGFF4r1+fPnF+tVd1NldKSUuqpHROy1117F+lFHHdWTNfXSP/zDPxTr//N//s/K56yxxhrF+vnnn1+sH3LIId0vDIC+WnvttYv1Jn+3+cpXvlKs/+EPf+j6WNA2l19++bCXQAVXJAAAAAC1GSQAAAAAtRkkAAAAALUZJAAAAAC1GSQAAAAAtY27a0NK6XURcV5EvDoiFkXErJzz6SmltSLi2xExLSLuj4h9c86P92+pk1fVzgkHHHBA5XOqdmeYNm1aL5Y0pptvvrlYP/nkk4v1yy67rJ/LmdRGPT85567qEdV5+NKXvlSsn3322cX67373u8oeW2+9dbF+8MEHF+ubb755sf7a1762WH/wwQcre1fdXbjqrt1UG/X8UF/VTjBvetObivWf/vSn/VzOUk926jvnnHOK9WWW6d3/tf3kJz/p2bHoP/lZurzzne8c9hKoUOdPyecj4h9zzhtHxNYR8ZGU0iYRcVxEXJVz3igirup8DPw5+YHm5AeakR1oTn6ghnEHCTnn+TnnWzvvL4iIuyJivYjYMyLO7XzauRFR3hweWkx+oDn5gWZkB5qTH6inq+u2UkrTImKLiLgxItbNOc+PWBy4iHhVrxcHo0R+oDn5gWZkB5qTH6g27j0SXpRSWjUiLoqIj+Wcn6z6ecXC82ZGxMxmy4PRID/QnPxAM7IDzckPjK3WFQkppeVjcZDOzzl/p1N+OKU0tfP41Ih4pPTcnPOsnPOMnPOMXiwYJhv5gebkB5qRHWhOfmB8dXZtSBFxVkTclXM+bYmHLouIQyPilM7bS/uywqXQuuuuW6xvsskmxfoZZ5xRrL/lLW/p2Zqq3HjjjcX6P//zP1c+59JLy7+VixYt6sma2kR+Xm7ZZZct1j/84Q8X6+973/uK9SeffLKyx0YbbdT9wgqq7rR9zTXXVD7nk5/8ZE96Iz/8SdVOML28s/4okZ2Xmz59erG+8847F+tVf+dZuHBhZY9/+7d/K9YffvjhcVbH0kR+li5veMMbhr0EKtT50YZ3RMTBEfHzlNJtndrxsThEF6aUPhARD0bEPv1ZIkxq8gPNyQ80IzvQnPxADeMOEnLO10dE1Q8F7dTb5cBokR9oTn6gGdmB5uQH6nFNIAAAAFCbQQIAAABQm0ECAAAAUJtBAgAAAFBbnV0bRtpaa61VrH/ta1+rfE7VFkKD2J6kaju6U089tVi//PLLi/VnnnmmZ2uivW644YZi/aabbqp8zlZbbdVVj1e/+tXFetU2rGP53e9+V6xfcMEFxfpRRx3VdQ9gcLbZZptiffbs2YNdCEu9V77ylcV61WtMlV//+teVjx199NFdHQsY33XXXVesV23/a7v6wXFFAgAAAFCbQQIAAABQm0ECAAAAUJtBAgAAAFCbQQIAAABQ28jt2vCXf/mXxfoxxxxTrL/97W8v1tdbb72eranK008/XfnYl770pWL9s5/9bLH+1FNP9WRN0I25c+cW6+9973srn/PBD36wWD/hhBN6sqaIiNNPP71YP/PMM4v1e++9t2e9gd5LKQ17CQAMwZw5c4r1e+65p1iv2kVvww03rOzx6KOPdr8wXJEAAAAA1GeQAAAAANRmkAAAAADUZpAAAAAA1GaQAAAAANQ2crs27L333l3Vm7jzzjuL9e9973vF+vPPP1+sn3rqqZU9nnjiie4XBkuJ+fPnVz524okndlUH2uMHP/hBsb7PPvsMeCWMmrvvvrtY/8lPflKsb7fddv1cDjBBVTvZfeMb3yjWTz755MpjffSjHy3Wq/7Nx2KuSAAAAABqM0gAAAAAajNIAAAAAGozSAAAAABqM0gAAAAAaks557E/IaXXRcR5EfHqiFgUEbNyzqenlE6MiMMj4tHOpx6fc/7+OMcauxkshXLOqelz5Ye2kx9orml+ZAfilpzzjCZPlJ/JYfXVVy/WL7zwwmJ95513rjzWd77znWL9/e9/f7H+1FNPjbO6ya3ua0+d7R+fj4h/zDnfmlJaLSJuSSld2XnsX3LOX2y6SGgB+YHm5AeakR1oTn6ghnEHCTnn+RExv/P+gpTSXRGxXr8XBqNAfqA5+YFmZAeakx+op6t7JKSUpkXEFhFxY6d0RErpZymls1NKa/Z4bTBS5Aeakx9oRnagOfmBarUHCSmlVSPiooj4WM75yYg4MyI2jIjpsXhqd2rF82amlG5OKd3cg/XCpCQ/0Jz8QDOyA83JD4xt3JstRkSklJaPiO9FxOU559MKj0+LiO/lnDcd5zhuOMKkM5GbxUXID+0mP9DcBG9WKju0WeObLUbIz2TgZov9U/e1Z9wrElJKKSLOioi7lgxSSmnqEp+2d0TM6XaRMOrkB5qTH2hGdqA5+YF66mz/uF1EXBcRP4/FW6BERBwfEQfE4kt7ckTcHxEf7NycZKxjmcox6Uzwf4Tkh1aTH2huAts/yg5tN5HtH+VnEqu6UuHkk0+ufM6HPvShYn2zzTYr1u+8887uFzaJ9Gz7x5zz9RFROtiY+6YC8gMTIT/QjOxAc/ID9XS1awMAAADQbgYJAAAAQG0GCQAAAEBtBgkAAABAbePu2tDTZu5cyiQ0kbvO95L8MBnJDzS3NORHdpikGu/a0Evyw2RU97XHFQkAAABAbQYJAAAAQG0GCQAAAEBtBgkAAABAbQYJAAAAQG3LDbjfbyPigc77UzofD4Peete1fi8XMkHyo/dk6ys/L9fG3m085170XlryIzt6T8be8vPn9Na7rtrZGej2j3/WOKWbh7Uti956T3Zt/Xrq3Y6+/dbG38th9m7jOQ+7d7+09eupd7t690tbv556j25vP9oAAAAA1GaQAAAAANQ2zEHCLL31bkHvfmnr11PvdvSHhqAmAAAfsElEQVTttzb+Xg6zdxvPedi9+6WtX0+929W7X9r69dR7RHsP7R4JAAAAwOTjRxsAAACA2gwSAAAAgNoMEgAAAIDaDBIAAACA2gwSAAAAgNoMEgAAAIDaDBIAAACA2gwSAAAAgNoMEgAAAIDaDBIAAACA2gwSAAAAgNoMEgAAAIDaDBIAAACA2gwSAAAAgNoMEgAAAIDaDBIAAACA2gwSAAAAgNoMEgAAAIDaDBIAAACA2gwSAAAAgNoMEgAAAIDaDBIAAACA2gwSAAAAgNoMEgAAAIDaDBIAAACA2gwSAAAAgNoMEgAAAIDaDBIAAACA2gwSAAAAgNoMEgAAAIDaDBIAAACA2gwSAAAAgNoMEgAAAIDaDBIAAACA2gwSAAAAgNoMEgAAAIDaDBIAAACA2gwSAAAAgNoMEgAAAIDaDBIAAACA2gwSAAAAgNqGMkhIKb0rpfSLlNK9KaXjBtz7/pTSz1NKt6WUbu5zr7NTSo+klOYsUVsrpXRlSumezts1B9j7xJTSrzvnfltKabc+9H1dSumalNJdKaU7UkpHdep9P+8xevf9vAelLdnp9BtKfoaVnU4f+emjtuSnja89nT7y00fyIz/9OPc2ZCdCfkY5P61+7ck5D/RXRCwbEfdFxBsiYoWIuD0iNhlg//sjYsqAem0fEVtGxJwlal+IiOM67x8XEZ8fYO8TI+LoPp/z1IjYsvP+ahHxy4jYZBDnPUbvvp/3gL6fWpOdTr+h5GdY2en0kZ/+fW1bk582vvZ0+shP/7628iM/fTn3Uc9O57zkZ4Tz0+bXnmFckfD2iLg35/yrnPPCiLggIvYcwjr6Lud8bUQ89pLynhFxbuf9cyNirwH27ruc8/yc862d9xdExF0RsV4M4LzH6D0qWpOdiOHlZ1jZ6fSWn/5pTX7a+NrT6S0//SM/8hPRh3NvQXYi5Gek89Pm155hDBLWi4iHlvh4bgz2D4wcEVeklG5JKc0cYN8XrZtznh+x+Dc/Il414P5HpJR+1rn8py+XFr0opTQtIraIiBtjwOf9kt4RAzzvPmp7diKGm5+Bfg/JT8+1PT+tee2JkJ8+kB/56fu5j2h2IuSnNflp22vPMAYJqVDLA+z/jpzzlhHx7oj4SEpp+wH2HrYzI2LDiJgeEfMj4tR+NUoprRoRF0XEx3LOT/arT83eAzvvPpOd4Rno95D89IX8DI/8yM9Eyc+I52eEsxMhP8Pk3z59PO9hDBLmRsTrlvj4tRExb1DNc87zOm8fiYiLY/HlRoP0cEppakRE5+0jg2qcc3445/xCznlRRHw9+nTuKaXlY/E38/k55+90ygM571LvQZ33ALQ9OxFDys8gv4fkp2/anp+Rf+2JkJ8+kh/56du5j3h2IuRn5PPT1teeYQwSboqIjVJKG6SUVoiI/SPiskE0TimtklJa7cX3I2LXiJgz9rN67rKIOLTz/qERcemgGr/4zdyxd/Th3FNKKSLOioi7cs6nLfFQ38+7qvcgzntA2p6diCHlZ1DfQ/LTV23Pz0i/9nT6yE//yI/8RPTh3FuQnQj5Gen8tPq1Jw/gDp4v/RURu8Xiu0reFxGfGGDfN8TiO6XeHhF39Lt3RPxHLL6c5LlYPI38QESsHRFXRcQ9nbdrDbD3NyPi5xHxs1j8zT21D323i8WXa/0sIm7r/NptEOc9Ru++n/egfrUlO52eQ8nPsLLT6S0//f2eakV+2vja0+ktP/39vpIf+en5ubchO53zlJ8RzU+bX3tSZxEAAAAA4xrGjzYAAAAAk5RBAgAAAFCbQQIAAABQm0ECAAAAUJtBAgAAAFDb0AYJKaWZeus96r37pa1fT73b0bff2vh7OczebTznYfful7Z+PfVuV+9+aevXU+/R7T3MKxKG+QeE3npPdm39eurdjr791sbfy2H2buM5D7t3v7T166l3u3r3S1u/nnqPaO8JDRJSSu9KKf0ipXRvSum4Xi0K2kB+oDn5gebkB5qTH1gs5ZybPTGlZSPilxGxS0TMjYibIuKAnPOdYzynWTMYopxz6vUx5Ye2kB9obmnIj+wwSf0257xOrw8qP7RB3deeiVyR8PaIuDfn/Kuc88KIuCAi9pzA8aBN5Aeakx9oTn5ogwf6dFz5gY6JDBLWi4iHlvh4bqcGjE9+oDn5gebkB5qTH+hYbgLPLV3y8LLLdzp3jRzFG6bARMgPNCc/0Ny4+ZEdqCQ/0DGRQcLciHjdEh+/NiLmvfSTcs6zImJWhJ8TgiXIDzQnP9DcuPmRHagkP9AxkR9tuCkiNkopbZBSWiEi9o+Iy3qzLBh58gPNyQ80Jz/QnPxAR+MrEnLOz6eUjoiIyyNi2Yg4O+d8R89WBiNMfqA5+YHm5Aeakx/4k8bbPzZq5vIeJqF+bL/VhPwwGckPNLc05Ed2mKRuyTnPGPYi5IfJaBDbPwIAAAAtY5AAAAAA1GaQAAAAANRmkAAAAADUZpAAAAAA1GaQAAAAANRmkAAAAADUZpAAAAAA1GaQAAAAANRmkAAAAADUZpAAAAAA1GaQAAAAANRmkAAAAADUZpAAAAAA1GaQAAAAANRmkAAAAADUttywFwDQa6effnqxfuSRRxbrc+bMKdZ33333Yv2BBx5otjAAAOi46qqrivWUUrH+13/91/1cTldckQAAAADUZpAAAAAA1GaQAAAAANRmkAAAAADUZpAAAAAA1DahXRtSSvdHxIKIeCEins85z+jFouid1VZbrVhfddVVK5/zN3/zN8X6OuusU6yfdtppxfqzzz47zuraTX4mZtq0aZWPHXTQQcX6okWLivWNN964WH/LW95SrNu1YfjkZ2Le9KY3VT62/PLLF+vbb799sf6Vr3ylWK/K2yBceumlxfr+++9f+ZyFCxf2azlLHfnpj6rsbLvttsX6Zz/72WL9He94R8/WRO/JD936l3/5l8rHqv58OO+88/q1nJ7pxfaPf5Vz/m0PjgNtJD/QnPxAc/IDzckPredHGwAAAIDaJjpIyBFxRUrplpTSzF4sCFpEfqA5+YHm5Aeakx+Iif9owztyzvNSSq+KiCtTSnfnnK9d8hM6ARMyeDn5gebkB5obMz+yA2OSH4gJXpGQc57XeftIRFwcEW8vfM6snPMMNyKBPyc/0Jz8QHPj5Ud2oJr8wGKNr0hIKa0SEcvknBd03t81Ij7ds5VRVHWn+mOPPbZY32abbYr1TTfdtFdLiqlTpxbrRx55ZM96jBr5mbhHH3208rFrr722WN9jjz36tRwGSH5e7q1vfWuxfthhhxXr++yzT+Wxllmm/H8Mr3nNa4r1qt0Zcs6VPfqtKutf/epXK5/zsY99rFh/8skne7KmpYX89M8aa6xRrF9zzTXF+m9+85ti/dWvfnVXn8/gyA9jOeWUU4r1//7f/3vlc5577rli/aqrrurJmvppIj/asG5EXJxSevE4/55z/q+erApGn/xAc/IDzckPNCc/0NF4kJBz/lVEbN7DtUBryA80Jz/QnPxAc/IDf2L7RwAAAKA2gwQAAACgNoMEAAAAoLaJ3GyRCXrLW95S+VjV3aMPPPDAYn2llVYq1js3g3mZhx56qLL3ggULivWNN964WN93332L9a985SvF+t13313ZG+p66qmnKh974IEHBrgSGL7Pfe5zxfpuu+024JUs3Q455JDKx84666xi/cc//nG/lkPLVe3OYNcGmJy23nrrYn355ZevfM71119frF944YU9WVM/uSIBAAAAqM0gAQAAAKjNIAEAAACozSABAAAAqM0gAQAAAKjNIAEAAACozfaPPbTGGmsU65///OeL9f3226/yWKuttlpP1nTPPfcU6+985zsrn1O1RUnVto1Tpkzpqg698MpXvrLysc0333yAK4Hhu/LKK4v1Jts/PvLII8V61faIyyxT/j+JRYsWdd172223LdZ32GGHro8FS7uqLbqhjbbffvti/ROf+ESxfsABBxTrjz32WM/WVKWq96abblqs33fffZXHOvroo3uypmFwRQIAAABQm0ECAAAAUJtBAgAAAFCbQQIAAABQm0ECAAAAUJtdG3po7733Ltb//u//vu+9q+4GussuuxTrDz30UOWx3vjGN/ZkTdBPK6+8cuVjr3/963vSY6uttirWq3YwiYh44IEHetIbunHmmWcW65dccknXx3ruueeK9d/85jddH6tbq6++erE+Z86cYv01r3lNV8cf6+tx8803d3UsmKicc7H+ile8YsArgeGbNWtWsb7RRhsV65tsskmxfv311/dsTVWOP/74Yn3ttdcu1g8//PDKY91+++09WdMwuCIBAAAAqM0gAQAAAKjNIAEAAACozSABAAAAqM0gAQAAAKht3F0bUkpnR8TuEfFIznnTTm2tiPh2REyLiPsjYt+c8+P9W+bksM8++/TsWPfff3+xftNNNxXrxx57bLE+1u4MVTbeeOOun0OZ/PTPvHnzKh+bPXt2sX7iiSd21aPq85944onK55xxxhld9aCa/NT3/PPPF+tNXgOG6Z3vfGexvuaaa/bk+HPnzq187Nlnn+1Jj6WF/ExeM2bMKNZ/+tOfDngl7SU/g/f0008X68Pc3WT69OnF+vrrr1+sL1q0qFgf1Z1Y6lyRMDsi3vWS2nERcVXOeaOIuKrzMfBys0N+oKnZIT/Q1OyQH2hqdsgPjGncQULO+dqIeOwl5T0j4tzO++dGxF49XheMBPmB5uQHmpMfaE5+YHxN75Gwbs55fkRE5+2rerckGHnyA83JDzQnP9Cc/MASxr1HwkSllGZGxMx+94FRJD/QnPxAM7IDzckPbdH0ioSHU0pTIyI6bx+p+sSc86yc84ycc/nOMdA+8gPNyQ80Vys/sgNF8gNLaHpFwmURcWhEnNJ5e2nPVjSJHX744cX6zJnloeQVV1xReax77723WH/kkcq/M/fMuuuu2/ceLSc/fXbSSScV693u2sBSSX4muf3337/ysarX0ZVWWqknvT/5yU/25DiTmPz0UdXuKb///e+L9TXWWKNY33DDDXu2JnpKfiao6u9nERF/8Rd/Uazfddddxfrtt9/ekzVFRKyyyirFetWueCuvvHKxXrWzyn/+5382W9hSbtwrElJK/xERN0TEm1NKc1NKH4jFAdolpXRPROzS+Rh4CfmB5uQHmpMfaE5+YHzjXpGQcz6g4qGderwWGDnyA83JDzQnP9Cc/MD4mt4jAQAAAGghgwQAAACgNoMEAAAAoDaDBAAAAKC2pts/UjBv3rxifbJtObfNNtsMewnQF8ssU56dLlq0aMArgdFx4IEHFuvHHXdcsf7GN76x8ljLL798T9Z02223FevPPfdcT44PJU888USxft111xXru+++ez+XA0Pzute9rliv2uI3onr71COOOKJYf/TRR7tfWIXTTjutWN9nn32K9ap/873jHe/o2ZomA1ckAAAAALUZJAAAAAC1GSQAAAAAtRkkAAAAALUZJAAAAAC12bVhkjnyyCOL9VVWWaVnPf7iL/6iq8//yU9+UqzfcMMNvVgO9EzV7gw55wGvBHpr2rRpxfrBBx9crO+88849673ddtsV673M1ZNPPlmsV+0M8f3vf79Yf+aZZ3q2JoC223TTTYv1iy++uFifMmVK5bG+/OUvF+s/+tGPul9YwdFHH1352GGHHdbVsU4++eQJrmY0uCIBAAAAqM0gAQAAAKjNIAEAAACozSABAAAAqM0gAQAAAKjNrg0DsPLKKxfrm2yySeVzPvWpTxXru+22W1e9l1mmPCuqunv9WObNm1esv//97y/WX3jhha57AFCt6g7Zl112WbH++te/vp/LGZjrrruuWJ81a9aAVwL9t/baaw97CbTQcstV/7PwoIMOKtbPOuusYr3Jvz+22WabYv3jH/94sX7aaacV62uttVaxvs8++1T2TikV6+edd16x/rWvfa3yWG3iigQAAACgNoMEAAAAoDaDBAAAAKA2gwQAAACgNoMEAAAAoLZxd21IKZ0dEbtHxCM55007tRMj4vCIeLTzacfnnL/fr0UubZZffvlifYsttijWL7roomJ96tSplT2eeeaZYr1q54QbbrihWH/Xu95VrFftJDGWqru5vve97y3WTz/99GJ94cKFXfeerOQHmpOf+qruOF1V76Ve7g5UZffddy/W3/3udxfrP/jBD3rWe7KSn8lrjz32GPYSWq+N+dl///0rH/vGN75RrOeci/WqP//vvffeyh4zZszoqr7nnnsW6+utt16xPta/ux599NFi/e/+7u8qn0O9KxJmR0TpX6P/knOe3vk1MiGCHpsd8gNNzQ75gaZmh/xAU7NDfmBM4w4Scs7XRsRjA1gLjBz5gebkB5qTH2hOfmB8E7lHwhEppZ+llM5OKa3ZsxVBO8gPNCc/0Jz8QHPyAx1NBwlnRsSGETE9IuZHxKlVn5hSmplSujmldHPDXjBq5Aeakx9orlZ+ZAeK5AeW0GiQkHN+OOf8Qs55UUR8PSLePsbnzso5z8g5l++UAS0jP9Cc/EBzdfMjO/By8gN/btxdG0pSSlNzzvM7H+4dEXN6t6SlwworrFD5WNVOCN/5zne66vFP//RPlY9dffXVxfqPf/zjYn2ttdbq6jibbrrpOKt7uXXWWadY/9znPlesP/jgg8X6JZdcUtnj2Wef7Xpdk00b8rO06tXd5bfffvvKx84444yujkV32p6fOXPKp7vjjjsW6wcddFCxfvnll1f2+OMf/9j1urrxgQ98oPKxj370o33t3XZtz8+wXHPNNcV61W4kLJ1GJT/77bdfsX7OOedUPue5554r1p944oli/W//9m+L9ccff7yyx6mnli8w3GGHHYr1qt0cqnYrqtphIiJiypQpxfpDDz1UrFe95t53332VPUZRne0f/yMidoyIKSmluRHxqYjYMaU0PSJyRNwfER/s4xph0pIfaE5+oDn5gebkB8Y37iAh53xAoXxWH9YCI0d+oDn5gebkB5qTHxjfRHZtAAAAAFrGIAEAAACozSABAAAAqM0gAQAAAKgtjbUVRs+bpTS4ZjUtv/zyxfqnP/3pyuccc8wxXfX4wQ9+UKwffPDBlc+p2k6lagvG73//+8X6lltuWawvXLiwsvcXvvCFYr1qy8g999yz8lgl/+f//J/Kxz7/+c8X62NtF1Ny2223dfX5Y8k5l/eRGbClMT+TzQsvvFCs9/LPwc0226xYv/POO3vWYzKRH15qjTXWqHzsd7/7XVfHes973lOsV73uTjZLQ35kZ+Le9773Fev/63/9r2L9mWeeKdY32WSTyh4PPPBA9wsbbbfknMv7Aw7Q0pifqq3h119//crnfOYznynWx9oysltV399f+9rXivVtttmmWG+y/WOVf//3fy/WDznkkK6PNZnUfe1xRQIAAABQm0ECAAAAUJtBAgAAAFCbQQIAAABQm0ECAAAAUNtyw17AoCy77LLF+kknnVSsH3300ZXHeuqpp4r14447rli/4IILivWqnRkiImbMKN9o9owzzijWt9hii2L9nnvuKdY/9KEPVfa+5pprivXVV1+9WN92222L9QMPPLBY32OPPSp7X3nllZWPlTz00EPF+gYbbNDVcWiHr371q8X6Bz/4wZ71mDlzZrH+sY99rGc9YDJ75zvfOewlwEA9//zzXX1+1V3nV1xxxV4sh5a79NJLi/XvfOc7lc+p+vt2L02ZMqVYr9o1rsoBBxxQrM+ZM6frNc2dO7fr57SJKxIAAACA2gwSAAAAgNoMEgAAAIDaDBIAAACA2gwSAAAAgNpas2tD1Z3Uq3ZnePrppyuPVXWH9yuuuKJY33rrrYv197///ZU93v3udxfrK620UrH+6U9/ulg/55xzivUmd1998skni/X/+q//6qpedTfViIi//du/7WpN//AP/9DV59Nud99997CXAP+/5ZdfvljfddddK59z9dVXF+vPPPNMT9bUS1WvcaeffvqAVwLDVXWX/KrXpLe85S3F+li7/3z4wx/ufmG00jD/DF5jjTUqH9tnn32K9apd4+67775i/cILL+x+YTTiigQAAACgNoMEAAAAoDaDBAAAAKA2gwQAAACgNoMEAAAAoLaUcx77E1J6XUScFxGvjohFETEr53x6SmmtiPh2REyLiPsjYt+c8+PjHGvsZn00f/78Yn2dddYp1p999tnKY1XdZXeVVVYp1t/4xjeOs7r6TjzxxGL9c5/7XLH+wgsv9Kx3W+WcU9Pnjkp+Rt0vf/nLYn3DDTfs+ljLLFOez1b9OVB11+FRIT8R2223XbH+iU98oljfZZddKo+1wQYbFOtNduLp1lprrVWs77bbbsX6l7/85WJ9tdVW67p31a4Ue+yxR7F+zTXXdN1jadQ0P6OSnVH3r//6r8V61Y4n6667buWx/vjHP/ZkTSPklpzzjCZPlJ/++fjHP1752EknnVSsP/roo8X6VlttVazPnTu3+4XxZ+q+9tS5IuH5iPjHnPPGEbF1RHwkpbRJRBwXEVflnDeKiKs6HwN/Tn6gOfmBZmQHmpMfqGHcQULOeX7O+dbO+wsi4q6IWC8i9oyIczufdm5E7NWvRcJkJT/QnPxAM7IDzckP1NPVPRJSStMiYouIuDEi1s05z49YHLiIeFWvFwejRH6gOfmBZmQHmpMfqLZc3U9MKa0aERdFxMdyzk+mVO/H9lJKMyNiZrPlwWiQH2hOfqAZ2YHm5AfGVuuKhJTS8rE4SOfnnL/TKT+cUpraeXxqRDxSem7OeVbOeUbTG57AZCc/0Jz8QDOyA83JD4xv3CsS0uLx21kRcVfO+bQlHrosIg6NiFM6by/tywp75De/+U2xXrVrw4orrlh5rM0337yr3t///veL9WuvvbbyOZdcckmxfv/99xfrdmdYOo1KfkbdHXfcUay/4Q1v6PpYixYtmuhy6BiV/JxxxhnF+qabbtr1sf7H//gfxfqCBQu6Pla3qnaT2HLLLYv18XaFKvnhD39YrJ955pnF+qjsztBro5KdtqrKzsKFCwe8knaSn4lbf/31i/W///u/r3xO1ff9rFmzinW7MwxfnR9teEdEHBwRP08p3dapHR+LQ3RhSukDEfFgROzTnyXCpCY/0Jz8QDOyA83JD9Qw7iAh53x9RFT9UNBOvV0OjBb5gebkB5qRHWhOfqCernZtAAAAANrNIAEAAACozSABAAAAqM0gAQAAAKitzq4NI2H77bcv1vfaa69ivWo7q4iIRx4pbhsbZ599drH++OOPF+u28YGlQ9XWQu95z3sGvBIY24c+9KFhL6G2qtfK7373u5XPOeqoo4r1P/7xjz1ZE0wGq6++erG+5557Vj7n4osv7tdyoGtXXnllsV61LWRExLe+9a1i/VOf+lRP1kTvuSIBAAAAqM0gAQAAAKjNIAEAAACozSABAAAAqM0gAQAAAKitNbs2LFiwoFj/5je/2VUdGD133nlnsX7XXXcV6xtvvHE/l8OIOeyww4r1j370o8X6oYce2sfVjO++++4r1p9++uli/brrrivWq3ZDmTNnTrOFwYjZd999i/Vnn322WK96TYKlzTnnnFOsn3TSSZXPufTSS/u1HPrEFQkAAABAbQYJAAAAQG0GCQAAAEBtBgkAAABAbQYJAAAAQG0p5zy4ZikNrhn0SM45DXsNEfLD5CQ/1VZcccVivWqXh4iIz3zmM8X6mmuuWaxfcsklxfqVV15Z2aPqztm/+c1vKp9DfywN+VkaszMqLrjggmK9amegPfbYo/JYDzzwQE/WNEJuyTnPGPYi5IfJqO5rjysSAAAAgNoMEgAAAIDaDBIAAACA2gwSAAAAgNoMEgAAAIDaxt21IaX0uog4LyJeHRGLImJWzvn0lNKJEXF4RDza+dTjc87fH+dY7lzKpDORu2bLD20nP9Bc0/zIDjTftUF+aLu6rz11BglTI2JqzvnWlNJqEXFLROwVEftGxB9yzl+suyhhYjKa4D+E5IdWkx9obgKDBNmh7SYySJAfWq3ua89yNQ40PyLmd95fkFK6KyLWm9jyoB3kB5qTH2hGdqA5+YF6urpHQkppWkRsERE3dkpHpJR+llI6O6W0Zo/XBiNFfqA5+YFmZAeakx+oVnuQkFJaNSIuioiP5ZyfjIgzI2LDiJgei6d2p1Y8b2ZK6eaU0s09WC9MSvIDzckPNCM70Jz8wNjGvUdCRERKafmI+F5EXJ5zPq3w+LSI+F7OedNxjuPnhJh0JvIz3hHyQ7vJDzQ3wXuMyA5t1vgeCRHyQ7vVfe0Z94qElFKKiLMi4q4lg9S5EcmL9o6IOd0uEkad/EBz8gPNyA40Jz9QT51dG7aLiOsi4uexeAuUiIjjI+KAWHxpT46I+yPig52bk4x1LFM5Jp0J/o+Q/NBq8gPNTWDXBtmh7Saya4P80Go92/6xl4SJyWiil2b3ivwwGckPNLc05Ed2mKQm9KMNvSI/TEY9+9EGAAAAgBcZJAAAAAC1GSQAAAAAtRkkAAAAALUZJAAAAAC1GSQAAAAAtRkkAAAAALUZJAAAAAC1GSQAAAAAtRkkAAAAALUtN+B+v42IBzrvT+l8PAx6613X+r1cyATJj96Tra/8vFwbe7fxnHvRe2nJj+zoPRl7y8+f01vvumpnJ+WcG/aYmJTSzTnnGXrrPcq9+6WtX0+929G339r4eznM3m0852H37pe2fj31blfvfmnr11Pv0e3tRxsAAACA2gwSAAAAgNqGOUiYpbfeLejdL239eurdjr791sbfy2H2buM5D7t3v7T166l3u3r3S1u/nnqPaO+h3SMBAAAAmHz8aAMAAABQm0ECAAAAUJtBAgAAAFCbQQIAAABQm0ECAAAAUNv/B2nNjvGjvlGSAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 1296x576 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# MNIST 데이터셋을 간략히 살펴보겠습니다.\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.gray() # 그레이스케일로 만들어줍니다.\n",
    "figures, axes = plt.subplots(nrows=2, ncols=5)\n",
    "figures.set_size_inches(18, 8)\n",
    "\n",
    " # train set 10개만 보겠습니다.\n",
    "print(y_train[0:10])\n",
    "\n",
    "axes[0][0].matshow(X_train[0])\n",
    "axes[0][1].matshow(X_train[1])\n",
    "axes[0][2].matshow(X_train[2])\n",
    "axes[0][3].matshow(X_train[3])\n",
    "axes[0][4].matshow(X_train[4])\n",
    "axes[1][0].matshow(X_train[5])\n",
    "axes[1][1].matshow(X_train[6])\n",
    "axes[1][2].matshow(X_train[7])\n",
    "axes[1][3].matshow(X_train[8])\n",
    "axes[1][4].matshow(X_train[9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Cross Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = X_train.reshape(60000, 28 * 28)\n",
    "X_test = X_test.reshape(10000, 28 * 28)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Solving MNIST problem using Multi-layer Neural Network with Mini Batch and Batch Normalization\n",
    "\n",
    "Single hidden layer with 1000 nodes with sigmoid activation and Batch Normalization applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(x, a = 0.1): #make it a leaky one\n",
    "    x = x * (x > 0) + x * (x <= 0) * a\n",
    "    return x\n",
    "\n",
    "def batch_normalize(z, e = 10 ** -12):\n",
    "    s = z.var(axis = 0, keepdims = True) ## axis = 1 takes variance along datas.\n",
    "    m = z.mean(axis = 0, keepdims = True) ## becomes (nodes, 1)\n",
    "    h_norm = (z - m)/(s + e)**(1/2)\n",
    "    return h_norm, s, m\n",
    "\n",
    "def sigmoid(x):\n",
    "    x = 1 / (1 + np.exp(-x))\n",
    "    return x\n",
    "\n",
    "def softmax(x):\n",
    "    x = np.exp(x) / np.exp(x).sum(axis = 1, keepdims = True)\n",
    "    return x\n",
    "\n",
    "def dSigmoid(y):\n",
    "    return (y * (1 - y))\n",
    "\n",
    "def dReLU(y, a = 0.1):\n",
    "    return 1 * (y > 0) + (y <= 0) / a\n",
    "\n",
    "def dSoftmax(y_hat, y):\n",
    "    return y_hat - y\n",
    "\n",
    "def dBatch_Norm(h, m, s, h_norm, scale, batch_size, dy, e = 10 ** -12):\n",
    "    dh_norm = dy * scale\n",
    "    ds = (dh_norm.T.dot(h - m)*(-0.5)*(s + e) ** (-3/2)).mean(axis = 0, keepdims = True)\n",
    "    dm = dh_norm * (-(s + e)**(-1/2))\n",
    "    dh = dh_norm * (s + e) ** (-1/2) + ds * 2 / batch_size * (h - m) + dm / batch_size\n",
    "#     dh = (dh_norm * (s + e) ** (-1/2)) * (1 - 1 / batch_size) + (dh_norm.T.dot(h - m)*(-0.5)*(s + e) ** (-3/2)).mean(axis = 0, keepdims = True) * 2 / batch_size * (h - m)\n",
    "    dscale = dh.T.dot(h_norm).mean(axis = 0, keepdims = True)\n",
    "    dshift = dh.mean(axis = 0, keepdims = True)\n",
    "    return dh, dscale, dshift\n",
    "\n",
    "def dLayer(W, z, dh):\n",
    "    db = dh.mean(axis = 0, keepdims = True)\n",
    "    dW = z.T.dot(dh)\n",
    "    dz = dh.dot(W.T)\n",
    "    return dW, db, dz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10.0, -0.1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dReLU(-1), ReLU(-1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehotencoding(y):\n",
    "    y = y.reshape((-1, 1))\n",
    "    i = np.hstack(list(1 * (y == x) for x in range(10)))\n",
    "    return i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stepDecay(lr): \n",
    "    return(lr/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Momentum_update(M, d, alpha = 0.9):\n",
    "    M = alpha * M + (1 - alpha) * d\n",
    "    return M\n",
    "\n",
    "def RMSprop_update(G, d, beta = 0.9):\n",
    "    G = beta * G + (1 - beta) * d ** 2\n",
    "    return G\n",
    "\n",
    "def Adam_update(M, G, adam_cnt, alpha = 0.9, beta = 0.9, e = 10 ** -12):\n",
    "    M = M / (1 - alpha ** adam_cnt)\n",
    "    G = G / (1 - beta ** adam_cnt)\n",
    "    d = M / (np.sqrt(G) + e)\n",
    "    return d\n",
    "\n",
    "def Adam_first_update(M, G, alpha = 0.9, beta = 0.9, e = 10 ** -12):\n",
    "    d = M / (np.sqrt(G) + e)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Adam(gradients, adamcache, adam_cnt, alpha = 0.9, beta = 0.9):\n",
    "    dW1, dW2, dW3, db1, db2, db3 = gradients['dW1'], gradients['dW2'], gradients['dW3'], gradients['db1'], gradients['db2'] , gradients['db3']\n",
    "    dscale1, dshift1, dscale2, dshift2 = gradients['dscale1'], gradients['dshift1'], gradients['dscale2'], gradients['dshift2']\n",
    "    MW1, GW1, Mb1, Gb1 = adamcache['MW1'], adamcache['GW1'], adamcache['Mb1'], adamcache['Gb1']\n",
    "    MW2, GW2, Mb2, Gb2 = adamcache['MW2'], adamcache['GW2'], adamcache['Mb2'], adamcache['Gb2']\n",
    "    MW3, GW3, Mb3, Gb3 = adamcache['MW3'], adamcache['GW3'], adamcache['Mb3'], adamcache['Gb3']\n",
    "    Mscale1, Mscale2, Mshift1, Mshift2 = adamcache['Mscale1'], adamcache['Mscale2'], adamcache['Mshift1'], adamcache['Mshift2']\n",
    "    Gscale1, Gscale2, Gshift1, Gshift2 = adamcache['Gscale1'], adamcache['Gscale2'], adamcache['Gshift1'], adamcache['Gshift2']\n",
    "    \n",
    "    MW1 = Momentum_update(MW1, dW1)\n",
    "    MW2 = Momentum_update(MW2, dW2)\n",
    "    MW3 = Momentum_update(MW3, dW3)\n",
    "    GW1 = RMSprop_update(GW1, dW1)\n",
    "    GW2 = RMSprop_update(GW2, dW2)\n",
    "    GW3 = RMSprop_update(GW3, dW3)\n",
    "    Mb1 = Momentum_update(Mb1, db1)\n",
    "    Mb2 = Momentum_update(Mb2, db2)\n",
    "    Mb3 = Momentum_update(Mb3, db3)\n",
    "    Gb1 = RMSprop_update(Gb1, db1)\n",
    "    Gb2 = RMSprop_update(Gb2, db2)\n",
    "    Gb3 = RMSprop_update(Gb3, db3)\n",
    "    Mscale1 = Momentum_update(Mscale1, dscale1)\n",
    "    Mscale2 = Momentum_update(Mscale2, dscale2)\n",
    "    Mshift1 = Momentum_update(Mshift1, dshift1)\n",
    "    Mshift2 = Momentum_update(Mshift2, dshift2)\n",
    "    Gscale1 = RMSprop_update(Gscale1, dscale1)\n",
    "    Gscale2 = RMSprop_update(Gscale2, dscale2)\n",
    "    Gshift1 = RMSprop_update(Gshift1, dshift1)\n",
    "    Gshift2 = RMSprop_update(Gshift2, dshift2) \n",
    "    \n",
    "    if adam_cnt > 0:\n",
    "        dW1 = Adam_update(MW1, GW1, adam_cnt)\n",
    "        db1 = Adam_update(Mb1, Gb1, adam_cnt)\n",
    "        dW2 = Adam_update(MW2, GW2, adam_cnt)\n",
    "        db2 = Adam_update(Mb2, Gb2, adam_cnt)\n",
    "        dW3 = Adam_update(MW3, GW3, adam_cnt)\n",
    "        db3 = Adam_update(Mb3, Gb3, adam_cnt)\n",
    "        dscale1 = Adam_update(Mscale1, Gscale1, adam_cnt)\n",
    "        dscale2 = Adam_update(Mscale2, Gscale2, adam_cnt)\n",
    "        dshift1 = Adam_update(Mshift1, Gshift1, adam_cnt)\n",
    "        dshift2 = Adam_update(Mshift2, Gshift2, adam_cnt)\n",
    "        \n",
    "    else:\n",
    "        dW1 = Adam_first_update(MW1, GW1)\n",
    "        db1 = Adam_first_update(Mb1, Gb1)\n",
    "        dW2 = Adam_first_update(MW2, GW2)\n",
    "        db2 = Adam_first_update(Mb2, Gb2)\n",
    "        dW3 = Adam_first_update(MW3, GW3)\n",
    "        db3 = Adam_first_update(Mb3, Gb3)\n",
    "        dscale1 = Adam_first_update(Mscale1, Gscale1)\n",
    "        dscale2 = Adam_first_update(Mscale2, Gscale2)\n",
    "        dshift1 = Adam_first_update(Mshift1, Gshift1)\n",
    "        dshift2 = Adam_first_update(Mshift2, Gshift2)\n",
    "    \n",
    "    gradients['dW3'] = dW3\n",
    "    gradients['db3'] = db3\n",
    "    gradients['dW2'] = dW2\n",
    "    gradients['db2'] = db2\n",
    "    gradients['dW1'] = dW1\n",
    "    gradients['db1'] = db1\n",
    "    gradients['dscale1'] = dscale1\n",
    "    gradients['dshift1'] = dshift1\n",
    "    gradients['dscale2'] = dscale2\n",
    "    gradients['dshift2'] = dshift2\n",
    "    \n",
    "    adamcache = {'MW1': MW1, 'GW1': GW1, 'Mb1': Mb1, 'Gb1': Gb1, 'MW2' : MW2, 'GW2' : GW2, 'Mb2': Mb2, 'Gb2' : Gb2, 'MW3' : MW3, 'Mb3' : Mb3, 'GW3' : GW3, 'Gb3' : Gb3, 'Mscale1':Mscale1, 'Mscale2':Mscale2, 'Mshift1':Mshift1, 'Mshift2':Mshift2, 'Gscale1':Gscale1, 'Gscale2':Gscale2, 'Gshift1':Gshift1, 'Gshift2':Gshift2}    \n",
    "    return gradients, adamcache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(model, X):\n",
    "\n",
    "    W1, b1, W2, b2, W3, b3 = model['W1'], model['b1'], model['W2'], model['b2'], model['W3'], model['b3']\n",
    "    shift1, scale1, shift2, scale2 = model['shift1'], model['scale1'], model['shift2'], model['scale2']\n",
    "    \n",
    "    h1 = X.dot(W1) + b1\n",
    "    h1_norm, s1, m1 = batch_normalize(h1)\n",
    "    y1 = h1_norm * scale1 + shift1\n",
    "    z1 = sigmoid(y1)\n",
    "    \n",
    "    h2 = z1.dot(W2) + b2\n",
    "    h2_norm, s2, m2 = batch_normalize(h2)\n",
    "    y2 = h2_norm * scale2 + shift2\n",
    "    z2 = sigmoid(y2)\n",
    "    \n",
    "    h3 = z2.dot(W3) + b3\n",
    "    y_hat = softmax(h3)\n",
    "\n",
    "    cache = {'h1': h1, 'z1': z1, 'h2': h2, 'z2': z2, 'h3': h3, 'y_hat': y_hat, 's1' : s1, 's2': s2, 'm1': m1, 'm2':m2, 'h1_norm' : h1_norm, 'h2_norm':h2_norm}\n",
    "\n",
    "    return y_hat, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_propagation(model, cache, adamcache, adam_cnt, X, y, batch_size, alpha = 0.9, beta = 0.9, e = 10 ** -12):\n",
    "    W1, b1, W2, b2, W3, b3 = model['W1'], model['b1'], model['W2'], model['b2'], model['W3'], model['b3']\n",
    "    h1, z1, h2, z2, h3, y_hat = cache['h1'], cache['z1'], cache['h2'], cache['z2'], cache['h3'], cache['y_hat']\n",
    "    s1, s2, m1, m2, h1_norm, h2_norm, scale1, scale2 = cache['s1'], cache['s2'], cache['m1'], cache['m2'], cache['h1_norm'], cache['h2_norm'], model['scale1'], model['scale2']\n",
    "    ## Fill In Your Code Here ##\n",
    "    \n",
    "    dh3 = (y_hat - y)\n",
    "    dW3, db3, dz2 = dLayer(W3, z2, dh3)\n",
    "    dy2 = dz2 * dSigmoid(z2)\n",
    "    dh2, dscale2, dshift2 = dBatch_Norm(h2, m2, s2, h2_norm, scale2, batch_size, dy2)\n",
    "    dW2, db2, dz1 = dLayer(W2, z1, dh2)\n",
    "    dy1 = dz1 * dSigmoid(z1)\n",
    "    dh1, dscale1, dshift1 = dBatch_Norm(h1, m1, s1, h1_norm, scale1, batch_size, dy1)\n",
    "    db1 = dh1.mean(axis = 0, keepdims = True)\n",
    "    dW1 = X.T.dot(dh1)\n",
    "\n",
    "    gradients = dict()\n",
    "    gradients['dW3'] = dW3\n",
    "    gradients['db3'] = db3\n",
    "    gradients['dscale2'] = dscale2\n",
    "    gradients['dshift2'] = dshift2\n",
    "    gradients['dW2'] = dW2\n",
    "    gradients['db2'] = db2\n",
    "    gradients['dscale1'] = dscale1\n",
    "    gradients['dshift1'] = dshift1\n",
    "    gradients['dW1'] = dW1\n",
    "    gradients['db1'] = db1\n",
    "    \n",
    "#     print('pre Adam')\n",
    "#     for key in gradients.keys():\n",
    "#         print(key)\n",
    "#         print(gradients[key])\n",
    "#         print('---------------' * 10)\n",
    "        \n",
    "    gradients, adamcache = Adam(gradients, adamcache, adam_cnt, alpha, beta)\n",
    "    \n",
    "#     print('post Adam')\n",
    "#     for key in gradients.keys():\n",
    "#         print(key)\n",
    "#         print(gradients[key])\n",
    "#         print('---------------' * 10)\n",
    "        \n",
    "    return gradients, adamcache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(model, X, y, e = 10 ** -12):\n",
    "    y_hat, _ = forward_propagation(model, X)\n",
    "    total_loss = - np.sum(y * np.log(y_hat + e))\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, X):\n",
    "    y_hat, _ = forward_propagation(model, X)\n",
    "    prediction = np.argmax(y_hat, axis = 1)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randn_initialization(nn_input_dim, nn_hdim1, nn_hdim2, nn_output_dim):\n",
    "    W1 = np.random.randn(nn_input_dim, nn_hdim1)\n",
    "    b1 = np.zeros((1, nn_hdim1))\n",
    "    W2 = np.random.randn(nn_hdim1, nn_hdim2)\n",
    "    b2 = np.zeros((1, nn_hdim2))\n",
    "    W3 = np.random.randn(nn_hdim2, nn_output_dim)\n",
    "    b3 = np.zeros((1, nn_output_dim))\n",
    "    shift1 = np.zeros((1, nn_hdim1))\n",
    "    shift2 = np.zeros((1, nn_hdim2))\n",
    "    scale1 = np.random.randn(1, nn_hdim1)\n",
    "    scale2 = np.random.randn(1, nn_hdim2)\n",
    "\n",
    "    return W1, b1, W2, b2, W3, b3, shift1, scale1, shift2, scale2\n",
    "\n",
    "\n",
    "def const_initialization(nn_input_dim, nn_hdim1, nn_hdim2, nn_output_dim):\n",
    "    # Constant initialization. why problematic? - same features.\n",
    "    W1 = np.ones((nn_input_dim, nn_hdim1))\n",
    "    b1 = np.zeros((1, nn_hdim1))\n",
    "    W2 = np.ones((nn_hdim1, nn_hdim2))\n",
    "    b2 = np.zeros((1, nn_hdim2))\n",
    "    W3 = np.ones((nn_hdim2, nn_output_dim))\n",
    "    b3 = np.zeros((1, nn_output_dim))\n",
    "\n",
    "    return W1, b1, W2, b2, W3, b3\n",
    "\n",
    "def adam_initialization(nn_input_dim, nn_hdim1, nn_hdim2, nn_output_dim):\n",
    "    MW1 = np.zeros((nn_input_dim, nn_hdim1))\n",
    "    MW2 = np.zeros((nn_hdim1, nn_hdim2))\n",
    "    MW3 = np.zeros((nn_hdim2, nn_output_dim))\n",
    "    Mb1 = np.zeros((1, nn_hdim1))\n",
    "    Mb2 = np.zeros((1, nn_hdim2))\n",
    "    Mb3 = np.zeros((1, nn_output_dim))\n",
    "    GW1 = np.zeros((nn_input_dim, nn_hdim1))\n",
    "    GW2 = np.zeros((nn_hdim1, nn_hdim2))\n",
    "    GW3 = np.zeros((nn_hdim2, nn_output_dim))\n",
    "    Gb1 = np.zeros((1, nn_hdim1))\n",
    "    Gb2 = np.zeros((1, nn_hdim2))\n",
    "    Gb3 = np.zeros((1, nn_output_dim))\n",
    "    Mscale1 = np.zeros((1, nn_hdim1))\n",
    "    Mscale2 = np.zeros((1, nn_hdim2))\n",
    "    Mshift1 = np.zeros((1, nn_hdim1))\n",
    "    Mshift2 = np.zeros((1, nn_hdim2))\n",
    "    Gscale1 = np.zeros((1, nn_hdim1))\n",
    "    Gscale2 = np.zeros((1, nn_hdim2))\n",
    "    Gshift1 = np.zeros((1, nn_hdim1))\n",
    "    Gshift2 = np.zeros((1, nn_hdim2))\n",
    "    \n",
    "    return MW1, MW2, MW3, Mb1, Mb2, Mb3, GW1, GW2, GW3, Gb1, Gb2, Gb3, Mscale1, Mscale2, Mshift1, Mshift2, Gscale1, Gscale2, Gshift1, Gshift2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(X_train, y_train, X_validate, y_validate, nn_input_dim, nn_hdim1, nn_hdim2, nn_output_dim,\n",
    "                lr=0.001, epoch=50000, batch_size = 20, print_loss=False, init_type='randn', bias_correction=True, alpha = 0.9, beta = 0.9):\n",
    "\n",
    "    # Initialization\n",
    "    np.random.seed(0)\n",
    "    if init_type == 'randn':\n",
    "        W1, b1, W2, b2, W3, b3, shift1, scale1, shift2, scale2 = randn_initialization(nn_input_dim, nn_hdim1, nn_hdim2, nn_output_dim)\n",
    "    elif init_type == 'const':\n",
    "        W1, b1, W2, b2, W3, b3 = const_initialization(nn_input_dim, nn_hdim1, nn_hdim2, nn_output_dim)\n",
    "    \n",
    "    MW1, MW2, MW3, Mb1, Mb2, Mb3, GW1, GW2, GW3, Gb1, Gb2, Gb3, Mscale1, Mscale2, Mshift1, Mshift2, Gscale1, Gscale2, Gshift1, Gshift2 = adam_initialization(nn_input_dim, nn_hdim1, nn_hdim2, nn_output_dim)\n",
    "    \n",
    "    model = { 'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2, 'W3': W3, 'b3': b3, 'shift1':shift1, 'shift2':shift2, 'scale1':scale1, 'scale2':scale2}\n",
    "    adamcache = {'MW1': MW1, 'GW1': GW1, 'Mb1': Mb1, 'Gb1': Gb1, 'MW2' : MW2, 'GW2' : GW2, 'Mb2': Mb2, 'Gb2' : Gb2, 'MW3' : MW3, 'Mb3' : Mb3, 'GW3' : GW3, 'Gb3' : Gb3, 'Mscale1':Mscale1, 'Mscale2':Mscale2, 'Mshift1':Mshift1, 'Mshift2':Mshift2, 'Gscale1':Gscale1, 'Gscale2':Gscale2, 'Gshift1':Gshift1, 'Gshift2':Gshift2}\n",
    "    \n",
    "    training_loss = []\n",
    "    validation_loss = []\n",
    "    true_train = y_train.copy()\n",
    "    true_validate = y_validate.copy()\n",
    "    y_validate = onehotencoding(y_validate)\n",
    "    y_train = onehotencoding(y_train)\n",
    "    batch_count = int(y_train.shape[0] / batch_size)\n",
    "    decay_cnt = 1\n",
    "    decay_stat = 0\n",
    "    v_loss_best = 1\n",
    "    model_best = dict()\n",
    "    # cache = dict()\n",
    "    # Full batch gradient descent.\n",
    "    adam_cnt = 0\n",
    "    for i in range(epoch):\n",
    "        for sample_run in range(batch_count):\n",
    "            X = X_train[sample_run * batch_size : (sample_run + 1) * batch_size, :]\n",
    "            y = y_train[sample_run * batch_size : (sample_run + 1) * batch_size, :]\n",
    "\n",
    "            # Forward propagation\n",
    "            y_hat, cache = forward_propagation(model, X)\n",
    "\n",
    "            # Backpropagation\n",
    "            gradients, adamcache = back_propagation(model, cache, adamcache, adam_cnt, X, y, batch_size, alpha, beta)\n",
    "            adam_cnt += 1\n",
    "            # Parameter update\n",
    "\n",
    "            W1 -= lr * gradients['dW1']\n",
    "            b1 -= lr * gradients['db1']\n",
    "            W2 -= lr * gradients['dW2']\n",
    "            b2 -= lr * gradients['db2']\n",
    "            W3 -= lr * gradients['dW3']\n",
    "            b3 -= lr * gradients['db3']\n",
    "            shift1 -= lr * gradients['dshift1']\n",
    "            shift2 -= lr * gradients['dshift2']\n",
    "            scale1 -= lr * gradients['dscale1']\n",
    "            scale2 -= lr * gradients['dscale2']\n",
    "\n",
    "            # Assign new parameters\n",
    "            model = { 'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2, 'W3': W3, 'b3': b3, 'shift1':shift1, 'shift2':shift2, 'scale1':scale1, 'scale2':scale2}\n",
    "        t_loss = compute_loss(model, X_train, y_train)\n",
    "        training_loss.append(t_loss)\n",
    "        v_loss = compute_loss(model, X_validate, y_validate)\n",
    "        validation_loss.append(v_loss)\n",
    "        print(f'iteration {i} - t_loss: {t_loss}, v_loss: {v_loss}')\n",
    "        y_predict = predict(model, X_train)\n",
    "        accuracy = np.equal(y_predict, true_train).mean()\n",
    "        print(f'Train Accuracy: {accuracy}')\n",
    "        if v_loss_best > v_loss:\n",
    "            v_loss_best = v_loss\n",
    "            model_best = model.copy()\n",
    "            \n",
    "        if i > 10 and abs(validation_loss[-1] - validation_loss[-2]) / validation_loss[-1] < 0.01:\n",
    "            decay_stat += 1\n",
    "            if decay_stat % 5 == 0:\n",
    "                print(f'{decay_cnt}th step decay')\n",
    "                decay_cnt += 1\n",
    "                lr = stepDecay(lr)\n",
    "                decay_stat = 0\n",
    "        if np.isnan(t_loss) or np.isnan(v_loss):\n",
    "            for key in model.keys():\n",
    "                print(key)\n",
    "                print(model[key])\n",
    "                print('----------------'*10)\n",
    "            return model, training_loss\n",
    "        \n",
    "        for key in model.keys():\n",
    "            if np.any(np.isnan(model[key])) or np.any(np.isinf(model[key])): \n",
    "                for key in model.keys():\n",
    "                    print(key)\n",
    "                    print(model[key])\n",
    "                    print('----------------'*10)\n",
    "                return model, training_loss\n",
    "        \n",
    "        if i > 2 and (abs(validation_loss[-2] - validation_loss[-1]) / validation_loss[-1] < 10 ** (-6) or accuracy > 0.9999):\n",
    "            break\n",
    "\n",
    "    return model, model_best, training_loss, validation_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate (Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "a = np.array([1,2,3,0.5,0.2, np.nan,50])\n",
    "print(np.any(np.isnan(a)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 - t_loss: 14453.400485605274, v_loss: 850.2784623199166\n",
      "Train Accuracy: 0.9213214285714286\n",
      "iteration 1 - t_loss: 11720.374204129124, v_loss: 678.4504525331454\n",
      "Train Accuracy: 0.9353035714285715\n",
      "iteration 2 - t_loss: 11181.589194746137, v_loss: 664.3048513357935\n",
      "Train Accuracy: 0.9377142857142857\n",
      "iteration 3 - t_loss: 14007.738567972405, v_loss: 787.3943482277673\n",
      "Train Accuracy: 0.9261607142857143\n",
      "iteration 4 - t_loss: 10249.050717905826, v_loss: 601.6179509715581\n",
      "Train Accuracy: 0.9440178571428571\n",
      "iteration 5 - t_loss: 13128.498785467602, v_loss: 785.4486398822474\n",
      "Train Accuracy: 0.9288928571428572\n",
      "iteration 6 - t_loss: 9636.345051870176, v_loss: 608.525819185265\n",
      "Train Accuracy: 0.9482142857142857\n",
      "iteration 7 - t_loss: 7246.984471976075, v_loss: 551.399744918954\n",
      "Train Accuracy: 0.960375\n",
      "iteration 8 - t_loss: 6277.448859155679, v_loss: 539.7741841123561\n",
      "Train Accuracy: 0.9660357142857143\n",
      "iteration 9 - t_loss: 6762.943726301962, v_loss: 600.253266446668\n",
      "Train Accuracy: 0.9653928571428572\n",
      "iteration 10 - t_loss: 6917.965200964982, v_loss: 611.9196490439047\n",
      "Train Accuracy: 0.9636785714285714\n",
      "iteration 11 - t_loss: 6384.17284129717, v_loss: 570.5383971440452\n",
      "Train Accuracy: 0.9670892857142858\n",
      "iteration 12 - t_loss: 7000.681575268679, v_loss: 682.200569663726\n",
      "Train Accuracy: 0.9642142857142857\n",
      "iteration 13 - t_loss: 6850.511511741785, v_loss: 623.9278534173352\n",
      "Train Accuracy: 0.9654285714285714\n",
      "iteration 14 - t_loss: 6571.025350338879, v_loss: 594.8277947083714\n",
      "Train Accuracy: 0.9658035714285714\n",
      "iteration 15 - t_loss: 6127.819164290549, v_loss: 616.5913010205793\n",
      "Train Accuracy: 0.9676071428571429\n",
      "iteration 16 - t_loss: 6165.539806225567, v_loss: 610.9042354622102\n",
      "Train Accuracy: 0.9693571428571428\n",
      "iteration 17 - t_loss: 4753.598882424771, v_loss: 620.5673183613227\n",
      "Train Accuracy: 0.9775178571428571\n",
      "iteration 18 - t_loss: 4697.765898391459, v_loss: 619.6958999176297\n",
      "Train Accuracy: 0.9775178571428571\n",
      "iteration 19 - t_loss: 4225.361436842947, v_loss: 627.7249610951496\n",
      "Train Accuracy: 0.980375\n",
      "iteration 20 - t_loss: 4590.352995638571, v_loss: 674.4986627758055\n",
      "Train Accuracy: 0.9795714285714285\n",
      "iteration 21 - t_loss: 4793.372627084862, v_loss: 639.8085143759373\n",
      "Train Accuracy: 0.9786428571428571\n",
      "iteration 22 - t_loss: 4804.492885663953, v_loss: 766.4483464506536\n",
      "Train Accuracy: 0.978125\n",
      "iteration 23 - t_loss: 4213.033761780899, v_loss: 622.6307752797585\n",
      "Train Accuracy: 0.9815357142857143\n",
      "iteration 24 - t_loss: 4532.6508428594425, v_loss: 565.2178529781712\n",
      "Train Accuracy: 0.9805714285714285\n",
      "iteration 25 - t_loss: 5438.30182746805, v_loss: 639.0018697683446\n",
      "Train Accuracy: 0.97675\n",
      "iteration 26 - t_loss: 6384.467917224688, v_loss: 626.0258054915428\n",
      "Train Accuracy: 0.9732857142857143\n",
      "iteration 27 - t_loss: 5390.879688622308, v_loss: 556.7677121445312\n",
      "Train Accuracy: 0.9761964285714285\n",
      "iteration 28 - t_loss: 5387.596618442658, v_loss: 637.6128330607911\n",
      "Train Accuracy: 0.9765\n",
      "iteration 29 - t_loss: 4434.414650914987, v_loss: 622.8305242871332\n",
      "Train Accuracy: 0.9805178571428571\n",
      "iteration 30 - t_loss: 5994.619193302777, v_loss: 763.5011501900523\n",
      "Train Accuracy: 0.9764285714285714\n",
      "iteration 31 - t_loss: 4827.998910438194, v_loss: 772.4845836244879\n",
      "Train Accuracy: 0.9793392857142857\n",
      "iteration 32 - t_loss: 4902.718299830574, v_loss: 766.0278547370858\n",
      "Train Accuracy: 0.9793392857142857\n",
      "iteration 33 - t_loss: 4506.823879771179, v_loss: 708.431300364746\n",
      "Train Accuracy: 0.9806071428571429\n",
      "iteration 34 - t_loss: 4468.774733874655, v_loss: 647.4111188707996\n",
      "Train Accuracy: 0.9820357142857142\n",
      "iteration 35 - t_loss: 4620.520383534579, v_loss: 801.7223784178162\n",
      "Train Accuracy: 0.9816785714285714\n",
      "iteration 36 - t_loss: 3966.9510011888215, v_loss: 807.3674131374637\n",
      "Train Accuracy: 0.9834285714285714\n",
      "iteration 37 - t_loss: 4312.640794392575, v_loss: 843.0265108282256\n",
      "Train Accuracy: 0.9825535714285715\n",
      "iteration 38 - t_loss: 4553.5580837857515, v_loss: 804.7154214589807\n",
      "Train Accuracy: 0.9831964285714285\n",
      "iteration 39 - t_loss: 5079.452919827753, v_loss: 788.826268596644\n",
      "Train Accuracy: 0.9814642857142857\n",
      "iteration 40 - t_loss: 5958.164164592166, v_loss: 775.9965135459637\n",
      "Train Accuracy: 0.9775892857142857\n",
      "iteration 41 - t_loss: 6103.590621531437, v_loss: 773.1651977115384\n",
      "Train Accuracy: 0.9775178571428571\n",
      "1th step decay\n",
      "iteration 42 - t_loss: 4865.213778014374, v_loss: 737.0198814193594\n",
      "Train Accuracy: 0.9804285714285714\n",
      "iteration 43 - t_loss: 4592.256037932049, v_loss: 741.2546613518405\n",
      "Train Accuracy: 0.9817321428571428\n",
      "iteration 44 - t_loss: 4563.5885828459695, v_loss: 699.0701621887777\n",
      "Train Accuracy: 0.9817142857142858\n",
      "iteration 45 - t_loss: 4756.791564441107, v_loss: 778.475820061329\n",
      "Train Accuracy: 0.9812321428571429\n",
      "iteration 46 - t_loss: 4840.224700258911, v_loss: 776.8097425669632\n",
      "Train Accuracy: 0.9813392857142857\n",
      "iteration 47 - t_loss: 5013.919159496393, v_loss: 768.5383563666641\n",
      "Train Accuracy: 0.9805714285714285\n",
      "iteration 48 - t_loss: 5144.804198311795, v_loss: 754.7839052811933\n",
      "Train Accuracy: 0.9802321428571429\n",
      "iteration 49 - t_loss: 4943.379146927135, v_loss: 762.9203092472751\n",
      "Train Accuracy: 0.9807678571428572\n",
      "iteration 50 - t_loss: 4843.520365822992, v_loss: 736.2748613029946\n",
      "Train Accuracy: 0.981125\n",
      "iteration 51 - t_loss: 5026.665625276284, v_loss: 720.6976525524665\n",
      "Train Accuracy: 0.9810357142857142\n",
      "iteration 52 - t_loss: 5184.490521721555, v_loss: 814.1486553935679\n",
      "Train Accuracy: 0.9802678571428571\n",
      "iteration 53 - t_loss: 5175.715014594258, v_loss: 825.6886502257803\n",
      "Train Accuracy: 0.9801785714285715\n",
      "iteration 54 - t_loss: 5601.226903486578, v_loss: 839.7448108610924\n",
      "Train Accuracy: 0.9793928571428572\n",
      "iteration 55 - t_loss: 5646.3203454712375, v_loss: 857.7819260550822\n",
      "Train Accuracy: 0.9794464285714286\n",
      "iteration 56 - t_loss: 5163.300316534038, v_loss: 838.4790712940111\n",
      "Train Accuracy: 0.98125\n",
      "iteration 57 - t_loss: 5206.862528481541, v_loss: 859.582103104506\n",
      "Train Accuracy: 0.9811428571428571\n",
      "iteration 58 - t_loss: 5179.378894541765, v_loss: 859.9083040495282\n",
      "Train Accuracy: 0.9809642857142857\n",
      "iteration 59 - t_loss: 5419.401077428372, v_loss: 859.1566302600761\n",
      "Train Accuracy: 0.9799107142857143\n",
      "iteration 60 - t_loss: 5649.929773224179, v_loss: 870.8759488474942\n",
      "Train Accuracy: 0.9798035714285714\n",
      "iteration 61 - t_loss: 5318.500607272436, v_loss: 847.893751722872\n",
      "Train Accuracy: 0.9808392857142857\n",
      "iteration 62 - t_loss: 5527.00892375647, v_loss: 804.1571195016611\n",
      "Train Accuracy: 0.9804464285714286\n",
      "iteration 63 - t_loss: 5575.628131432725, v_loss: 828.9267881282626\n",
      "Train Accuracy: 0.9797142857142858\n",
      "iteration 64 - t_loss: 5456.179626947878, v_loss: 854.6493095622534\n",
      "Train Accuracy: 0.9806071428571429\n",
      "iteration 65 - t_loss: 5176.879414338733, v_loss: 773.4893325151045\n",
      "Train Accuracy: 0.9814821428571429\n",
      "iteration 66 - t_loss: 5654.254881083436, v_loss: 787.1986536490933\n",
      "Train Accuracy: 0.9804285714285714\n",
      "iteration 67 - t_loss: 5439.8313733785635, v_loss: 811.2253781215336\n",
      "Train Accuracy: 0.9816785714285714\n",
      "iteration 68 - t_loss: 5127.944655426784, v_loss: 786.7242925488849\n",
      "Train Accuracy: 0.9825178571428571\n",
      "iteration 69 - t_loss: 5242.236346675596, v_loss: 784.1248220572085\n",
      "Train Accuracy: 0.9824821428571429\n",
      "2th step decay\n",
      "iteration 70 - t_loss: 4547.564968996541, v_loss: 784.166663869815\n",
      "Train Accuracy: 0.9841964285714285\n",
      "iteration 71 - t_loss: 4440.915612322144, v_loss: 799.9677450063359\n",
      "Train Accuracy: 0.9845535714285715\n",
      "iteration 72 - t_loss: 4587.267796511918, v_loss: 791.824689641881\n",
      "Train Accuracy: 0.9847321428571428\n",
      "iteration 73 - t_loss: 4649.8020489244445, v_loss: 840.3495428250201\n",
      "Train Accuracy: 0.9847321428571428\n",
      "iteration 74 - t_loss: 4660.061491065892, v_loss: 823.8631378250839\n",
      "Train Accuracy: 0.9845714285714285\n",
      "iteration 75 - t_loss: 4761.216660391928, v_loss: 843.6123125273634\n",
      "Train Accuracy: 0.9844107142857143\n",
      "iteration 76 - t_loss: 4770.76710720195, v_loss: 874.5571242241112\n",
      "Train Accuracy: 0.9843035714285714\n",
      "iteration 77 - t_loss: 4920.921698286967, v_loss: 881.3724795917225\n",
      "Train Accuracy: 0.9841428571428571\n",
      "iteration 78 - t_loss: 4901.421233200236, v_loss: 895.2312883571822\n",
      "Train Accuracy: 0.9841607142857143\n",
      "iteration 79 - t_loss: 5157.794178615708, v_loss: 896.9860848008282\n",
      "Train Accuracy: 0.9836428571428572\n",
      "iteration 80 - t_loss: 5283.774378329379, v_loss: 915.9854190596848\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.9836785714285714\n",
      "iteration 81 - t_loss: 5419.18739594028, v_loss: 934.690195233924\n",
      "Train Accuracy: 0.9836785714285714\n",
      "iteration 82 - t_loss: 5699.282087057901, v_loss: 940.2378256077947\n",
      "Train Accuracy: 0.9832321428571429\n",
      "iteration 83 - t_loss: 5557.733040566623, v_loss: 939.7204157426156\n",
      "Train Accuracy: 0.9832857142857143\n",
      "3th step decay\n",
      "iteration 84 - t_loss: 5518.7278614432125, v_loss: 939.7077610873637\n",
      "Train Accuracy: 0.9836785714285714\n",
      "iteration 85 - t_loss: 5561.676391126071, v_loss: 966.3752603766193\n",
      "Train Accuracy: 0.9837678571428572\n",
      "iteration 86 - t_loss: 5638.4419725350635, v_loss: 994.5767542919781\n",
      "Train Accuracy: 0.9835892857142857\n",
      "iteration 87 - t_loss: 5712.972654411697, v_loss: 1012.5488133931727\n",
      "Train Accuracy: 0.9834642857142857\n",
      "iteration 88 - t_loss: 5773.952188993141, v_loss: 1019.9979173469935\n",
      "Train Accuracy: 0.9836071428571429\n",
      "iteration 89 - t_loss: 5842.077530146949, v_loss: 1033.5277284805304\n",
      "Train Accuracy: 0.9835535714285715\n",
      "iteration 90 - t_loss: 5910.624352641969, v_loss: 1051.5319816360811\n",
      "Train Accuracy: 0.9835714285714285\n",
      "iteration 91 - t_loss: 5982.227386462905, v_loss: 1063.6795834810437\n",
      "Train Accuracy: 0.9832142857142857\n",
      "iteration 92 - t_loss: 6051.153506293602, v_loss: 1079.2082468673295\n",
      "Train Accuracy: 0.9828928571428571\n",
      "iteration 93 - t_loss: 6119.052099932099, v_loss: 1091.9159295588993\n",
      "Train Accuracy: 0.9828035714285714\n",
      "iteration 94 - t_loss: 6171.4804507105155, v_loss: 1097.847736796056\n",
      "Train Accuracy: 0.9827321428571428\n",
      "iteration 95 - t_loss: 6154.774772814941, v_loss: 1112.3261054181455\n",
      "Train Accuracy: 0.9829464285714286\n",
      "iteration 96 - t_loss: 6155.285830825653, v_loss: 1125.1885553361426\n",
      "Train Accuracy: 0.9828035714285714\n",
      "iteration 97 - t_loss: 6241.2737327857285, v_loss: 1137.0892984978882\n",
      "Train Accuracy: 0.9825892857142857\n",
      "iteration 98 - t_loss: 6306.736289722331, v_loss: 1153.7689399493322\n",
      "Train Accuracy: 0.9823571428571428\n",
      "iteration 99 - t_loss: 6342.290597376424, v_loss: 1173.496252888233\n",
      "Train Accuracy: 0.9821071428571428\n",
      "iteration 100 - t_loss: 6441.965523329537, v_loss: 1177.5456851802142\n",
      "Train Accuracy: 0.9815714285714285\n",
      "iteration 101 - t_loss: 6460.373027697005, v_loss: 1176.6003325565002\n",
      "Train Accuracy: 0.9816071428571429\n",
      "4th step decay\n",
      "iteration 102 - t_loss: 6505.385072147702, v_loss: 1174.0469851194387\n",
      "Train Accuracy: 0.9817142857142858\n",
      "iteration 103 - t_loss: 6527.346616480587, v_loss: 1167.4541488258878\n",
      "Train Accuracy: 0.9818392857142857\n",
      "iteration 104 - t_loss: 6556.965299169943, v_loss: 1161.722355570318\n",
      "Train Accuracy: 0.9819107142857143\n",
      "iteration 105 - t_loss: 6575.792778988473, v_loss: 1158.0555813714222\n",
      "Train Accuracy: 0.9819285714285715\n",
      "iteration 106 - t_loss: 6581.290651060473, v_loss: 1156.6656367059968\n",
      "Train Accuracy: 0.9817857142857143\n",
      "5th step decay\n",
      "iteration 107 - t_loss: 6546.109482352239, v_loss: 1146.4796809771447\n",
      "Train Accuracy: 0.9819821428571428\n",
      "iteration 108 - t_loss: 6540.841863979108, v_loss: 1146.4102216003453\n",
      "Train Accuracy: 0.982\n",
      "iteration 109 - t_loss: 6532.307115369531, v_loss: 1147.1948284568548\n",
      "Train Accuracy: 0.9820357142857142\n",
      "iteration 110 - t_loss: 6520.929232734805, v_loss: 1148.4397279337607\n",
      "Train Accuracy: 0.9821785714285715\n",
      "iteration 111 - t_loss: 6509.279107502089, v_loss: 1149.9129592588406\n",
      "Train Accuracy: 0.9822321428571429\n",
      "6th step decay\n",
      "iteration 112 - t_loss: 6493.872151373075, v_loss: 1150.218392912844\n",
      "Train Accuracy: 0.9823214285714286\n",
      "iteration 113 - t_loss: 6490.393747921694, v_loss: 1152.263084749957\n",
      "Train Accuracy: 0.9822678571428571\n",
      "iteration 114 - t_loss: 6486.331946924746, v_loss: 1153.8567569164782\n",
      "Train Accuracy: 0.9823392857142857\n",
      "iteration 115 - t_loss: 6483.429616057362, v_loss: 1155.3889896450346\n",
      "Train Accuracy: 0.9822857142857143\n",
      "iteration 116 - t_loss: 6481.638389126804, v_loss: 1156.9004042131703\n",
      "Train Accuracy: 0.9823571428571428\n",
      "7th step decay\n",
      "iteration 117 - t_loss: 6478.4122777129505, v_loss: 1157.658924058976\n",
      "Train Accuracy: 0.9824464285714286\n",
      "iteration 118 - t_loss: 6478.4864173742635, v_loss: 1159.0194924011753\n",
      "Train Accuracy: 0.9824107142857142\n",
      "iteration 119 - t_loss: 6479.091348868952, v_loss: 1160.1220373383244\n",
      "Train Accuracy: 0.9824107142857142\n",
      "iteration 120 - t_loss: 6479.995213789594, v_loss: 1161.0103852552875\n",
      "Train Accuracy: 0.9823214285714286\n",
      "iteration 121 - t_loss: 6481.200249649795, v_loss: 1161.7998259627675\n",
      "Train Accuracy: 0.9823214285714286\n",
      "8th step decay\n",
      "iteration 122 - t_loss: 6481.82569225774, v_loss: 1161.9190826921522\n",
      "Train Accuracy: 0.9823035714285714\n",
      "iteration 123 - t_loss: 6482.722020038648, v_loss: 1162.3559125754243\n",
      "Train Accuracy: 0.9823571428571428\n",
      "iteration 124 - t_loss: 6483.856733935623, v_loss: 1162.8714382484334\n",
      "Train Accuracy: 0.9823392857142857\n",
      "iteration 125 - t_loss: 6485.163647532053, v_loss: 1163.3746010269006\n",
      "Train Accuracy: 0.9823035714285714\n",
      "iteration 126 - t_loss: 6486.602755052528, v_loss: 1163.8400023731588\n",
      "Train Accuracy: 0.9822857142857143\n",
      "9th step decay\n",
      "iteration 127 - t_loss: 6487.405461543013, v_loss: 1163.9402649279314\n",
      "Train Accuracy: 0.9822678571428571\n",
      "iteration 128 - t_loss: 6488.207256544421, v_loss: 1164.1065706671393\n",
      "Train Accuracy: 0.98225\n",
      "iteration 129 - t_loss: 6489.034169868307, v_loss: 1164.3093979619557\n",
      "Train Accuracy: 0.98225\n",
      "iteration 130 - t_loss: 6489.896008147246, v_loss: 1164.5309184597097\n",
      "Train Accuracy: 0.98225\n",
      "iteration 131 - t_loss: 6490.796418067552, v_loss: 1164.760235718291\n",
      "Train Accuracy: 0.9822321428571429\n",
      "10th step decay\n",
      "iteration 132 - t_loss: 6491.271485891288, v_loss: 1164.836459761649\n",
      "Train Accuracy: 0.9822142857142857\n",
      "iteration 133 - t_loss: 6491.744630773858, v_loss: 1164.9253549769387\n",
      "Train Accuracy: 0.9822321428571429\n",
      "iteration 134 - t_loss: 6492.21965254669, v_loss: 1165.0234580732022\n",
      "Train Accuracy: 0.98225\n",
      "iteration 135 - t_loss: 6492.698580338679, v_loss: 1165.1283751435838\n",
      "Train Accuracy: 0.9821964285714285\n",
      "iteration 136 - t_loss: 6493.1828654191695, v_loss: 1165.2382098305043\n",
      "Train Accuracy: 0.9822142857142857\n",
      "11th step decay\n",
      "iteration 137 - t_loss: 6493.432050210455, v_loss: 1165.283610948913\n",
      "Train Accuracy: 0.9822142857142857\n",
      "iteration 138 - t_loss: 6493.680838275754, v_loss: 1165.3315588776277\n",
      "Train Accuracy: 0.9822142857142857\n",
      "iteration 139 - t_loss: 6493.929837943814, v_loss: 1165.3816281056152\n",
      "Train Accuracy: 0.9822321428571429\n",
      "iteration 140 - t_loss: 6494.179334085474, v_loss: 1165.4335217434127\n",
      "Train Accuracy: 0.9822321428571429\n",
      "iteration 141 - t_loss: 6494.429581461956, v_loss: 1165.4869700871172\n",
      "Train Accuracy: 0.9822321428571429\n",
      "12th step decay\n",
      "iteration 142 - t_loss: 6494.556567404872, v_loss: 1165.511349200112\n",
      "Train Accuracy: 0.9822321428571429\n",
      "iteration 143 - t_loss: 6494.683496561588, v_loss: 1165.5362877589578\n",
      "Train Accuracy: 0.9822321428571429\n",
      "iteration 144 - t_loss: 6494.81047874648, v_loss: 1165.5617248417484\n",
      "Train Accuracy: 0.9822321428571429\n",
      "iteration 145 - t_loss: 6494.937551723853, v_loss: 1165.587624242828\n",
      "Train Accuracy: 0.9822321428571429\n",
      "iteration 146 - t_loss: 6495.0647497300315, v_loss: 1165.613951957113\n",
      "Train Accuracy: 0.9822321428571429\n",
      "13th step decay\n",
      "iteration 147 - t_loss: 6495.128825486621, v_loss: 1165.6265374796067\n",
      "Train Accuracy: 0.9822321428571429\n",
      "iteration 148 - t_loss: 6495.192892286016, v_loss: 1165.6392541480045\n",
      "Train Accuracy: 0.9822321428571429\n",
      "iteration 149 - t_loss: 6495.256971675759, v_loss: 1165.6520916945462\n",
      "Train Accuracy: 0.9822321428571429\n",
      "iteration 150 - t_loss: 6495.32106869201, v_loss: 1165.6650456222228\n",
      "Train Accuracy: 0.9822321428571429\n",
      "iteration 151 - t_loss: 6495.385187190388, v_loss: 1165.6781115805202\n",
      "Train Accuracy: 0.9822321428571429\n",
      "14th step decay\n",
      "iteration 152 - t_loss: 6495.417366271507, v_loss: 1165.6845011016255\n",
      "Train Accuracy: 0.9822321428571429\n",
      "iteration 153 - t_loss: 6495.449543584396, v_loss: 1165.6909223263212\n",
      "Train Accuracy: 0.9822321428571429\n",
      "iteration 154 - t_loss: 6495.481724091207, v_loss: 1165.6973733062327\n",
      "Train Accuracy: 0.9822321428571429\n",
      "iteration 155 - t_loss: 6495.513908321854, v_loss: 1165.7038534864914\n",
      "Train Accuracy: 0.9822321428571429\n",
      "iteration 156 - t_loss: 6495.546096849453, v_loss: 1165.7103623223122\n",
      "Train Accuracy: 0.9822321428571429\n",
      "15th step decay\n",
      "iteration 157 - t_loss: 6495.562221113619, v_loss: 1165.713581048931\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.9822321428571429\n",
      "iteration 158 - t_loss: 6495.578345009652, v_loss: 1165.7168075809782\n",
      "Train Accuracy: 0.9822321428571429\n",
      "iteration 159 - t_loss: 6495.594469692285, v_loss: 1165.7200415056038\n",
      "Train Accuracy: 0.9822321428571429\n",
      "iteration 160 - t_loss: 6495.61059522538, v_loss: 1165.7232827530133\n",
      "Train Accuracy: 0.9822321428571429\n",
      "iteration 161 - t_loss: 6495.626721668374, v_loss: 1165.7265312501117\n",
      "Train Accuracy: 0.9822321428571429\n",
      "16th step decay\n",
      "iteration 162 - t_loss: 6495.634792374413, v_loss: 1165.7281465948395\n",
      "Train Accuracy: 0.9822321428571429\n",
      "iteration 163 - t_loss: 6495.642862991446, v_loss: 1165.7297638722343\n",
      "Train Accuracy: 0.9822321428571429\n",
      "iteration 164 - t_loss: 6495.650933800124, v_loss: 1165.7313829869536\n",
      "Train Accuracy: 0.9822321428571429\n",
      "iteration 165 - t_loss: 6495.6590048128, v_loss: 1165.7330039300598\n",
      "Train Accuracy: 0.9822321428571429\n",
      "iteration 166 - t_loss: 6495.667076038368, v_loss: 1165.7346266925927\n",
      "Train Accuracy: 0.9822321428571429\n",
      "17th step decay\n",
      "iteration 167 - t_loss: 6495.67111352985, v_loss: 1165.7354358495602\n",
      "Train Accuracy: 0.9822321428571429\n"
     ]
    }
   ],
   "source": [
    "nn_input_dim = 28*28\n",
    "nn_hdim1 = 1000\n",
    "nn_hdim2 = 100\n",
    "nn_output_dim = 10\n",
    "lr = 0.01\n",
    "epoch = 5000\n",
    "batch_size = 32\n",
    "print_loss= True\n",
    "init_type= 'randn'\n",
    "bias_correction = True\n",
    "\n",
    "model, model_best, training_loss, validation_loss = build_model(X_train[:56000], y_train[:56000], X_train[56000:], y_train[56000:], nn_input_dim, nn_hdim1, nn_hdim2, nn_output_dim,\n",
    "                lr, epoch, batch_size, print_loss, init_type, bias_correction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_best' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-664892c176f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_best\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model_best' is not defined"
     ]
    }
   ],
   "source": [
    "model_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9635\n"
     ]
    }
   ],
   "source": [
    "y_hat, _ = forward_propagation(model, X_test)\n",
    "y_predict = y_hat.argmax(axis = 1)\n",
    "print(np.equal(y_predict, y_test).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the Train results on the Train Data.\n",
    "z1 = np.dot(w1, X_train[:, :10000]) + b1 ## shape = (nodes, datas)\n",
    "s1 = z1.var(axis = 1, keepdims = True) ## axis = 1 takes variance along datas.\n",
    "m1 = z1.mean(axis = 1, keepdims = True) ## becomes (nodes, 1)\n",
    "x_normalized = (z1 - m1)/(s1 + eps)**(1/2)\n",
    "y1 = x_normalized * g + b\n",
    "a1 = sigmoid(y1)\n",
    "z2 = np.dot(w2, a1) + b2\n",
    "a2 = sigmoid(z2)\n",
    "y_predict_hot = a2\n",
    "y_predict = np.argmax(y_predict_hot, axis=0)\n",
    "\n",
    "# actual vs. predict\n",
    "train_result = pd.DataFrame({'actual': y_train[:10000], 'predict': y_predict})\n",
    "\n",
    "# accuracy는 다음과 같이 계산됩니다.\n",
    "train_accuracy = (train_result[\"actual\"] == train_result[\"predict\"]).mean()\n",
    "print(\"Accuracy(train) = {0:.5f}\".format(train_accuracy))\n",
    "\n",
    "print(train_result.shape)\n",
    "train_result.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate (Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying on the Test Data\n",
    "\n",
    "# normally, test data's get evaluated 1 by 1, which makes calculating m1 and s1 impossible.\n",
    "# For this reason, we would get the average and variance of the training dataset and plug it into s1, m1.\n",
    "\n",
    "# However, for the MNIST, we have large number of test data which we plug in at the same time,\n",
    "# Thus we take mean and variance of the test dataset.\n",
    "\n",
    "z1 = np.dot(w1, X_test) + b1 ## shape = (nodes, datas)\n",
    "s1 = z1.var(axis = 1, keepdims = True) ## axis = 1 takes variance along datas.\n",
    "m1 = z1.mean(axis = 1, keepdims = True) ## becomes (nodes, 1)\n",
    "x_normalized = (z1 - m1)/(s1 + eps)**(1/2)\n",
    "y1 = x_normalized * g + b\n",
    "a1 = sigmoid(y1)\n",
    "z2 = np.dot(w2, a1) + b2\n",
    "a2 = sigmoid(z2)\n",
    "\n",
    "y_predict_hot = a2\n",
    "y_predict = np.argmax(y_predict_hot, axis=0)\n",
    "\n",
    "# actual vs. predict\n",
    "test_result = pd.DataFrame({'actual': y_test, 'predict': y_predict})\n",
    "\n",
    "# accuracy는 다음과 같이 계산됩니다.\n",
    "test_accuracy = (test_result[\"actual\"] == test_result[\"predict\"]).mean()\n",
    "print(\"Accuracy(test) = {0:.5f}\".format(test_accuracy))\n",
    "\n",
    "print(test_result.shape)\n",
    "test_result.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
