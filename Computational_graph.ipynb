{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Computational-graph.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arkincognito/ML-from-scrap/blob/master/Computational_graph.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tv99OtnSbdpn"
      },
      "source": [
        "## This notebook is based on an assignment from DS School's Deep Learning Course."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37k0KiDRaCND"
      },
      "source": [
        "Computational Graph enables Partial Derivation without explicitly representing the partial derivative.\n",
        "\n",
        "In this notebook, I'll write computational graph code for\n",
        "\n",
        "*Nodes(Log, Square, Trigonometric Functions)\n",
        "and\n",
        "*Loss Functions(Mean Square Error, Cross Entropy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6hbNz8baCNE"
      },
      "source": [
        "## Computational Graph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfn02vgDaCNE"
      },
      "source": [
        "Computational Graph는 represents the calculation process, where nodes represents calculation and edges represent input/outputs. Computational Graph visually represents chain rule and allows efficient calculation of derivitives.\n",
        "\n",
        "<img src=\"http://drive.google.com/uc?export=view&id=14x4zQpEEatgMb1W0BY47lXKjM5haZq1x\" width=\"600\">\n",
        "\n",
        "\n",
        "> **Forward Propagation**\n",
        ">-  **Forward propagation** goes through the neural network from the input nodes to output. **Forward propagation** is represented as blue edges in the graph above.\n",
        "  \n",
        "\n",
        "> **Back Propagation**\n",
        ">-  The process calculating gradient by applying chain rule through out the network. The term 'back' propagation comes from propagating the loss from the end of the network. **Back Propagaton** is represented as red edges in the graph above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RuxtY3XYaCNF"
      },
      "source": [
        "# 1. Multiply Node\n",
        "\n",
        "Multiply Node function is represented as $z=f(x,y)=x\\times y $.\n",
        "\n",
        "Then the gradient of $z$ can be written as  \n",
        "\n",
        "$$\n",
        "\\nabla z = \\left ( \\frac{\\partial z}{\\partial x},\\frac{\\partial z}{\\partial y}  \\right ) = \\left ( \\frac{\\partial (xy)}{\\partial x},\\frac{\\partial (xy)}{\\partial y}  \\right ) = (y,x)\n",
        "$$\n",
        "\n",
        "<img src=\"http://drive.google.com/uc?export=view&id=1pQ9HFmr9_31daD8YIhO72IQtr5Kvj2GP\" width=\"600\">\n",
        "\n",
        "\n",
        "> **Forward Propagation**\n",
        ">-  Multiply Node passes through the multiplied value of the two input values.\n",
        "  \n",
        "\n",
        "> **Back Propagation**\n",
        ">-  Multiply Node returns the multiplied value of gradient from the next node, and input value from the other input edge. Node from x will have y multiplied to the gradient."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0uqQOY1aCNG"
      },
      "source": [
        "# Define Multiply class and forward, back propagation methods.\n",
        "class Multiply:\n",
        "    # f(x,y) = xy\n",
        "    def forward(self, x, y):\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "        return self.x * self.y\n",
        "    # df/dx = y, df/dy = x\n",
        "    def backward(self):\n",
        "        dx = self.y\n",
        "        dy = self.x\n",
        "        return dx, dy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34gLNovEaCNK",
        "outputId": "44eca312-6af7-47df-be23-d3c6f5185e7f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "multiply=Multiply()\n",
        "forward2 = multiply.forward(10,3)\n",
        "forward2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J52qgUt1aCNR",
        "outputId": "deb03b7a-172c-4bb9-f5b2-c7bbff3c6ec1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "multiply.backward()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3, 10)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQUczXl1cfmp"
      },
      "source": [
        "More generally, multiplication can be written as $z = f(x_0, x_1, ... , x_{n-1}) = $$\\prod_{i=0}^{n-1} x_i$.\n",
        "\n",
        "Then the gradient of $z$ is:\n",
        "\n",
        "$$ \\nabla z = \\left ( \\frac{\\partial z}{\\partial x_0},\\frac{\\partial z}{\\partial x_1}, ... \\frac{\\partial z}{\\partial x_{n-1}} \\right ) = \\left ( \\frac{\\partial (\\prod_{i=0}^{n-1} x_i)}{\\partial x_0},\\frac{\\partial (\\prod_{i=0}^{n-1} x_i)}{\\partial x_1}, ... ,\\frac{\\partial (\\prod_{i=0}^{n-1} x_i)}{\\partial x_{n-1}}  \\right )$$\n",
        "\n",
        "Thus,\n",
        "\n",
        "$$ \\frac{\\partial z}{\\partial x_j} = \\frac{\\prod_{i=0}^{n-1} x_i} {x_j} $$\n",
        "\n",
        "Note that the input x now is the list of numbers we're solving for product."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLRYCP1gevym"
      },
      "source": [
        "class MultiplyAll:\n",
        "    def forward(self, array):\n",
        "        self.x = np.array(array)\n",
        "        return np.product(self.x)\n",
        "    def backward(self):\n",
        "        return np.array(list(np.prod(self.x) / i for i in self.x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPgiEBwDaCNV"
      },
      "source": [
        "# 2. Log Function Node"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JurMNDckaCNV"
      },
      "source": [
        "If $z=f(x)=log(x)$, then the gradient of $z$ is\n",
        "\n",
        "\n",
        "$$\n",
        "\\nabla z = \\left ( \\frac{\\partial z}{\\partial x}\\right ) = \\left ( \\frac{\\partial (log(x))}{\\partial x}\\right ) = \\frac{1}{x}\n",
        "$$\n",
        "\n",
        "<img src=\"http://drive.google.com/uc?export=view&id=1YKou4QvGFwjWV_zzk0H8cMS0vGT7lFMB\" width=\"600\">\n",
        "\n",
        "> **Forward Propagation**\n",
        ">-  Log node returns the log value of the input.\n",
        "\n",
        "> **Back Propagation**\n",
        ">-  Log node returns the inverse value of the input."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCF01NXtaCNW"
      },
      "source": [
        "# Import numpy to calculate log value.\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDWtIaNcaCNZ"
      },
      "source": [
        "# Define Log class and its forward, back propagation methods.\n",
        "class Log:\n",
        "    \n",
        "    # f(x) = log(x)\n",
        "    def forward(self, x):\n",
        "        self.x = x\n",
        "        return np.log(self.x)\n",
        "        \n",
        "    # df/dx = 1/x\n",
        "    def backward(self):\n",
        "        return 1.0/self.x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghBa9P5-aCNd"
      },
      "source": [
        "Let's create log node object, set **x=2** and see if forward, backward propagation values are returned correctly as **0.693** and **0.5**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5g7x8qhYaCNe",
        "outputId": "15da49ee-6a87-4c7d-81c3-0d9adb5bb7f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "log = Log()\n",
        "x = 2\n",
        "log.forward(x), log.backward()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.6931471805599453, 0.5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ltj5Oy5xaCNh"
      },
      "source": [
        "# 3. Square Node"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nz_balW9aCNh"
      },
      "source": [
        "If $z=f(x)=x^2$, then the gradient of $z$ is\n",
        "\n",
        "\n",
        "$$\n",
        "\\nabla z = \\left ( \\frac{\\partial z}{\\partial x}\\right ) = \\left ( \\frac{\\partial x^2}{\\partial x}\\right ) = 2x\n",
        "$$\n",
        "\n",
        "<img src=\"http://drive.google.com/uc?export=view&id=1C67JqOdnW4dxbLBVHLxF8Hvrr2NoTth3\" width=\"600\">\n",
        "\n",
        "\n",
        "> **Forward Propagation**\n",
        ">-  Square node returns the square value of the input.  \n",
        "\n",
        "> **Back Propagation**\n",
        ">-  Square node returns the multiplied value of $2x$ and the gradient."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NuXbxWmMaCNi"
      },
      "source": [
        "# Define Square class and its forward, back propagation methods.\n",
        "\n",
        "class Square:\n",
        "    \n",
        "    # f(x) = x^2\n",
        "    def forward(self, x):\n",
        "        self.x = x\n",
        "        return self.x ** 2\n",
        "    # df/dx = 2x\n",
        "    def backward(self):\n",
        "        return 2 * self.x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1pXnktfQaCNl",
        "outputId": "78e0ed09-552c-4ffe-d810-9dd518629514",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "square = Square()\n",
        "x =5\n",
        "square.forward(x), square.backward()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25, 10)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azQeSHUkaCNn"
      },
      "source": [
        "#4. Trigonometric Functions(sin, cos, tan)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_drsjry_aCNo"
      },
      "source": [
        "##4-1. Sin Node\n",
        "If $z=f(x)=sin(x)$, then the gradient of $z$ is\n",
        "\n",
        "\n",
        "$$\n",
        "\\nabla z = \\left ( \\frac{\\partial z}{\\partial x}\\right ) = \\left ( \\frac{\\partial sin(x)}{\\partial x}\\right ) = cos(x)\n",
        "$$\n",
        "<img src=\"http://drive.google.com/uc?export=view&id=1zdwScJP5hbMTtJETFVo1P9F8ZACoU-fc\" width=\"600\">\n",
        "\n",
        "> **Forward Propagation**\n",
        ">-  Sin node returns the sine value of the input.\n",
        "  \n",
        "\n",
        "> **Back Propagation**\n",
        ">-  Sin node returns multiplied value of the cosine value of the input and the gradient."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqYs08D-aCNo"
      },
      "source": [
        "# Define Sin class and its forward, back propagation methods.\n",
        "class Sin:\n",
        "    \n",
        "    def forward(self, x):\n",
        "        self.x = x\n",
        "        return np.sin(x)\n",
        "\n",
        "    def backward(self):\n",
        "        return np.cos(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNRSoqRiaCNu",
        "outputId": "081b4806-390d-40a6-e9fc-464c4610230a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "sin = Sin()\n",
        "x = np.pi / 3\n",
        "sin.forward(x), sin.backward()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.8660254037844386, 0.5000000000000001)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYzIZtXMaCNw"
      },
      "source": [
        "#4-2. Cos Node\n",
        "If $z=f(x)=cos(x)$, then the gradient of $z$ is\n",
        "\n",
        "\n",
        "$$\n",
        "\\nabla z = \\left ( \\frac{\\partial z}{\\partial x}\\right ) = \\left ( \\frac{\\partial cos(x)}{\\partial x}\\right ) = -sin(x)\n",
        "$$\n",
        "\n",
        "<img src=\"http://drive.google.com/uc?export=view&id=11AEEdSWOmBjGQhzW2-BDboXFjCvd-hi8\" width=\"600\">\n",
        "\n",
        "\n",
        "> **Forward Propagation**\n",
        ">-  Cos node returns the cosine value of the input.\n",
        "  \n",
        "\n",
        "> **Back Propagation**\n",
        ">-  Cos node returns multiplied value of the -sine value of the input and the gradient."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "msMjBRfVaCNx"
      },
      "source": [
        "class Cos:\n",
        "    def forward(self, x):\n",
        "        self.x = x\n",
        "        return np.cos(x)\n",
        "    \n",
        "    def backward(self):\n",
        "        return -np.sin(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-TAnRiqaCN1",
        "outputId": "c6b61970-9fbe-4a79-9394-b6abd67c9d2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cos = Cos()\n",
        "x = np.pi/3\n",
        "cos.forward(x), cos.backward()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.5000000000000001, -0.8660254037844386)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMYxbVeYaCN4"
      },
      "source": [
        "#4-3. Tan Node\n",
        "If $z=f(x)=tan(x)$, then the gradient of $z$ is\n",
        "\n",
        "\n",
        "$$\n",
        "\\nabla z = \\left ( \\frac{\\partial z}{\\partial x}\\right ) = \\left ( \\frac{\\partial tan(x)}{\\partial x}\\right ) = \\frac{1}{ cos(x)^2}\n",
        "$$\n",
        "\n",
        "<img src=\"http://drive.google.com/uc?export=view&id=16Q1LDQ4L2uY8dkqKgMRZP8UILcKtG10L\" width=\"600\">\n",
        "\n",
        "\n",
        "> **Forward Propagation**\n",
        ">- Tan node returns the tangent value of the input.\n",
        "  \n",
        "\n",
        "> **Back Propagation**\n",
        ">- Tan node multiplied value of the gradient and $\\frac{1}{ cos(x)^2}$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m5uA5dVxaCN4"
      },
      "source": [
        "class Tan:\n",
        "    \n",
        "    def forward(self, x):\n",
        "        self.x = x\n",
        "        return np.tan(x)\n",
        "    \n",
        "    def backward(self):\n",
        "        return 1 / (np.cos(x)**2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LbJdzx_MaCN7",
        "outputId": "d7595202-1f9a-45eb-8e09-49ed68932a2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tan = Tan()\n",
        "x = np.pi/3\n",
        "tan.forward(x), tan.backward()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1.7320508075688767, 3.9999999999999982)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8ug_8DLaCN-"
      },
      "source": [
        "### Let's define the loss funtions through computational graph and get the partial derivative of the loss funcions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZxeAqFndaCN_"
      },
      "source": [
        "#5. MSE Loss Function Node"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-A2vqK8aCN_"
      },
      "source": [
        "MSE(Mean Squared Error) Loss Function can be represented as following.\n",
        "\n",
        "$$\n",
        "MSE=\\frac{1}{2} (\\hat{y}-y)^2\n",
        "$$\n",
        "\n",
        "\n",
        "n represents the total number of data, $y^{(i)}$ represents label of i'th data.\n",
        "\n",
        " predicted value of ${y}^{(i)}$: $\\hat{y}^{(i)}=\\sigma(w^Tx^{(i)}+b)$\n",
        " and Cost Function of MSE can be represented as\n",
        "\n",
        "\n",
        "$$\n",
        "MSE(Cost)=\\frac{1}{n}\\sum_{i=1}^{n}\\frac{1}{2} (\\hat{y}^{(i)}-y^{(i)})^2\n",
        "$$\n",
        "\n",
        "Cost Function is just an average of loss function, so I'll only define Loss Function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQePtNbtaCOA"
      },
      "source": [
        "Below shows the Computatioanl Graph of the MSE Loss Function above.\n",
        "\n",
        "<img src=\"http://drive.google.com/uc?export=view&id=1dqfw6Tpeo6CkNsns8IMU3Bw1ItkAUNZ6\" width=\"600\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1X7zKIfaCOB"
      },
      "source": [
        "To find the optimal weights $w$ and bias $b$ for the Gradient Descent algorithm, we need the partial derivative values of the Loss by $w$ and $b$.  Thus, our MSE node should return the partial derivative value of $\\hat{y}$, $\\frac{\\partial L}{\\partial \\hat{y}} $\n",
        "\n",
        "MSE Node can be represented as following diagram.\n",
        "\n",
        "<img src=\"http://drive.google.com/uc?export=view&id=16dbnhIahsvpVh1eAF4uavMA8mZIMkWhF\" width=\"600\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWbTSEoLaCOB"
      },
      "source": [
        "Let's build a MSE Node\n",
        "\n",
        "When $\\hat{y}$ and $y$ are given as inputs, forward propagation will return $ MSE=\\frac{1}{2} (\\hat{y}-y)^2 $ and backward propagation will return  $\\frac{\\partial L}{\\partial \\hat{y}} $."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3vpAey6aCOC"
      },
      "source": [
        "class Add:\n",
        "    def forward(self, x, y):\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "        return x + y\n",
        "    def backward(self):\n",
        "        return 1.0, 1.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MfU74gvUaCOE"
      },
      "source": [
        "class MSE:\n",
        "    mult1 = Multiply()\n",
        "    mult2 = Multiply()\n",
        "    add = Add()\n",
        "    sq = Square()\n",
        "    def forward(self, y_predict, y):\n",
        "        self.y_predict = y_predict\n",
        "        self.y = y\n",
        "        fw1 = self.mult1.forward(self.y, -1)\n",
        "        fw2 = self.add.forward(fw1, self.y_predict)\n",
        "        fw3 = self.sq.forward(fw2)\n",
        "        fw4 = self.mult2.forward(fw3, 1/2)\n",
        "        return fw4\n",
        "    \n",
        "    def backward(self):\n",
        "        bw1 = self.mult2.backward()[0]\n",
        "        bw2 = self.sq.backward() * bw1\n",
        "        bw3 = self.add.backward()[1] * bw2\n",
        "        return bw3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPsYHdqkaCOG"
      },
      "source": [
        "Create MSE node object, enter **y_predict = 1, y = 4** as input and check if the forward, backward values are **4.5, -3.0**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7w-M2YJ3aCOG",
        "outputId": "2e1ae3c9-4909-401c-b715-066f04652ddf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "mse = MSE()\n",
        "mse.forward(1,4), mse.backward()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4.5, -3.0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QM34LQNpaCOI"
      },
      "source": [
        "### 5. Cross Entropy Loss Function Node"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWJ5eRGPaCOJ"
      },
      "source": [
        "If total number of class(label) is C, $y_{c}$ represents c'th label value, predicted value of ${y}_{c}$ : $\\hat{y}_{c}=\\sigma(w^Tx_{c}+b)$, then the Cross Entropy Loss can be represented as following: \n",
        "\n",
        "$$\n",
        "\\text{Cross Entropy Loss} = -\\sum_{c=1}^{C}  y_{c} \\times log(\\hat{y}_{c})\n",
        "$$\n",
        "\n",
        "For Binary Classification problem, $C = 2$ thus the cross entropy can be represented as:\n",
        "\n",
        "$$ \\text{Cross Entropy Loss(binary)} = - y \\times log(\\hat{y}) - (1-{y}) \\times log(1-\\hat{y})$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_T8oWXQaCOJ"
      },
      "source": [
        "이번 과제에서는 Binary Classification이 아닌 Multi-Class, 즉, C=3인 Multi-Class Classification 의 Cross Entropy Loss를 Computational Graph로 그리고, 이 Cross Entropy Loss의 편미분을 구현해보도록 하겠습니다. \n",
        "\n",
        "C=3를 대입한 Cross Entropy Loss는 다음과 같고, \n",
        "\n",
        "$$\n",
        "\\text{Cross Entropy Loss} = -\\sum_{c=1}^{3}  y_{c} \\times log(\\hat{y}_{c})\n",
        "$$\n",
        "\n",
        "이를 Computational Graph로 그려주면 다음과 같습니다. \n",
        "\n",
        "<img src=\"http://drive.google.com/uc?export=view&id=1HFLacOYnnN5ihTN9pm1_LEL1CB-1JNny\" width=\"600\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJcPPAquaCOJ"
      },
      "source": [
        "이 Loss Function을 Gradient Descent 알고리즘에 적용하여 최적의 w와 b를 구해주기 위해서는 Loss 값을 w와 b로 편미분한 값을 필요로 했습니다. 따라서, 우리는 위 Cross Entropy의 Computational Graph에서 예측치인 $\\hat{y}$에 대한 Loss의 편미분 값. $\\frac{\\partial L}{\\partial \\hat{y}} $ 을 구해주어야 합니다. \n",
        "즉, 지금 우리가 만들어주려하는 CE 노드는 다음과 같이 나타낼 수 있습니다. \n",
        "\n",
        "<img src=\"http://drive.google.com/uc?export=view&id=15wLtFHEr884R75PZrl7TLTUj2izJkWM-\" width=\"600\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfg42r1haCOK"
      },
      "source": [
        "이번 5번 과제는 위와 같이 CE(Cross Entropy) 노드에 $\\hat{y}$과 $y$를 입력받았을 때, forward propagation으로 $ \\text{Cross Entropy Loss} = -\\sum_{c=1}^{3}  y_{c} \\times log(\\hat{y}_{c})$ 을 출력하고, backward propagation으로는 $\\frac{\\partial L}{\\partial \\hat{y}} $ 값을 return 해주는 CE 노드를 만드는 것입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qo9dJQ8CaCOK",
        "outputId": "5abb71fc-389b-4131-9828-5881c194c301",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "mult = Multiply()\n",
        "a = np.array([2,3,4])\n",
        "b = np.array([0,1,2])\n",
        "mult.forward(a,b), mult.backward()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([0, 3, 8]), (array([0, 1, 2]), array([2, 3, 4])))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ly0_xz-iaCON",
        "outputId": "ad19df7b-03c5-4c24-ebd5-9c76ca126d99",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "log = Log()\n",
        "a = np.array([1,2,10])\n",
        "log.forward(a), log.backward()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([0.        , 0.69314718, 2.30258509]), array([1. , 0.5, 0.1]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XndOZj7UaCOQ"
      },
      "source": [
        "class AddAll():\n",
        "    def forward(self, array):\n",
        "        self.array = np.array(array)\n",
        "        return self.array.sum()\n",
        "    \n",
        "    def backward(self):\n",
        "        return np.array([1 for i in range (self.array.shape[0])])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDQ-rvDQaCOS"
      },
      "source": [
        "# CE 노드에 적용되는\n",
        "# forward, back propagation 메소드를 정의합니다.\n",
        "class CE:\n",
        "    log = Log()\n",
        "    mult = Multiply()\n",
        "    addAll = AddAll()\n",
        "    lastMult = Multiply()\n",
        "    def forward(self, y_predict_ce, y_ce):\n",
        "        self.y_predict_ce = np.array(y_predict_ce)\n",
        "        self.y_ce = np.array(y_ce)\n",
        "        log_y_predict = self.log.forward(self.y_predict_ce)\n",
        "        log_yp_times_y = self.mult.forward(log_y_predict, self.y_ce)\n",
        "        added = self.addAll.forward(log_yp_times_y)\n",
        "        multiplied = self.lastMult.forward(added, -1)\n",
        "        return multiplied\n",
        "    \n",
        "    def backward(self):\n",
        "        bw1 = self.lastMult.backward()[0]\n",
        "        bw2 = bw1 * self.addAll.backward()\n",
        "        bw3 = bw2 * self.mult.backward()[0]\n",
        "        bw4 = bw3 * self.log.backward()\n",
        "        return bw4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPw6lgpUaCOV"
      },
      "source": [
        "CE 노드 객체를 생성하고, **y_predict_ce = [2, 3, 4] , y_ce = [0, 1, 2]** 값을 넣어준 후 forward, backward값이 각각 **-3.8712, (0, -0.33, -0.5)** 으로 잘 구해지는지 확인해 봅시다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FlkkAQYbaCOV",
        "outputId": "05031690-d595-4866-c042-77ddb6004e5b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "ce = CE()\n",
        "y_predict_ce = [2, 3, 4]\n",
        "y_ce = [0, 1, 2]\n",
        "print(f'Cross Entropy: {ce.forward(y_predict_ce, y_ce)}\\nBackward Propagation: {ce.backward()}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cross Entropy: -3.8712010109078907\n",
            "Backward Propagation: [ 0.         -0.33333333 -0.5       ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxPZ6sEXiL6T"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}