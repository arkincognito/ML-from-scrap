{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Computational-graph.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arkincognito/ML-from-scrap/blob/master/Computational_graph.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tv99OtnSbdpn"
      },
      "source": [
        "## This notebook is based on an assignment from DS School's Deep Learning Course."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCF01NXtaCNW"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37k0KiDRaCND"
      },
      "source": [
        "Computational Graph enables Partial Derivation without explicitly representing the partial derivative.\n",
        "\n",
        "In this notebook, I'll write computational graph code for\n",
        "\n",
        "*Nodes(Log, Square, Trigonometric Functions)\n",
        "and\n",
        "*Loss Functions(Mean Square Error, Cross Entropy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6hbNz8baCNE"
      },
      "source": [
        "## Computational Graph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfn02vgDaCNE"
      },
      "source": [
        "Computational GraphëŠ” represents the calculation process, where nodes represents calculation and edges represent input/outputs. Computational Graph visually represents chain rule and allows efficient calculation of derivitives.\n",
        "\n",
        "<img src=\"http://drive.google.com/uc?export=view&id=14x4zQpEEatgMb1W0BY47lXKjM5haZq1x\" width=\"600\">\n",
        "\n",
        "\n",
        "> **Forward Propagation**\n",
        ">-  **Forward propagation** goes through the neural network from the input nodes to output. **Forward propagation** is represented as blue edges in the graph above.\n",
        "  \n",
        "\n",
        "> **Back Propagation**\n",
        ">-  The process calculating gradient by applying chain rule through out the network. The term 'back' propagation comes from propagating the loss from the end of the network. **Back Propagaton** is represented as red edges in the graph above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RuxtY3XYaCNF"
      },
      "source": [
        "# 1. Multiply Node\n",
        "\n",
        "Multiply Node function is represented as $z=f(x,y)=x\\times y $.\n",
        "\n",
        "Then the gradient of $z$ can be written as  \n",
        "\n",
        "$$\n",
        "\\nabla z = \\left ( \\frac{\\partial z}{\\partial x},\\frac{\\partial z}{\\partial y}  \\right ) = \\left ( \\frac{\\partial (xy)}{\\partial x},\\frac{\\partial (xy)}{\\partial y}  \\right ) = (y,x)\n",
        "$$\n",
        "\n",
        "<img src=\"http://drive.google.com/uc?export=view&id=1pQ9HFmr9_31daD8YIhO72IQtr5Kvj2GP\" width=\"600\">\n",
        "\n",
        "\n",
        "> **Forward Propagation**\n",
        ">-  Multiply Node passes through the multiplied value of the two input values.\n",
        "  \n",
        "\n",
        "> **Back Propagation**\n",
        ">-  Multiply Node returns the multiplied value of gradient from the next node, and input value from the other input edge. Node from x will have y multiplied to the gradient."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0uqQOY1aCNG"
      },
      "source": [
        "# Define Multiply class and forward, back propagation methods.\n",
        "class Multiply:\n",
        "    # f(x,y) = xy\n",
        "    def forward(self, x, y):\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "        return self.x * self.y\n",
        "    # df/dx = y, df/dy = x\n",
        "    def backward(self):\n",
        "        dx = self.y\n",
        "        dy = self.x\n",
        "        return dx, dy"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJtvJsiHkPrb"
      },
      "source": [
        "Let's check if the class works properly.\n",
        "\n",
        "If the input values are 10 and 3, then the return should be:\n",
        "* Forward: $10 \\times 3 = 30$\n",
        "* Backward: $3, 10$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34gLNovEaCNK",
        "outputId": "d56806fe-982a-4028-c5e3-201248902048",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "multiply = Multiply()\n",
        "forward2 = multiply.forward(10,3)\n",
        "forward2"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J52qgUt1aCNR",
        "outputId": "dc12e838-9f51-4033-d778-d21cf4a68ef2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "multiply.backward()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3, 10)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yi-FaN5IlGNI"
      },
      "source": [
        "We can see that the Node works fine."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQUczXl1cfmp"
      },
      "source": [
        "##1-1 Multiply in General\n",
        "More generally, multiplication can be written as $z = f(x_0, x_1, ... , x_{n-1}) = $$\\prod_{i=0}^{n-1} x_i$.\n",
        "\n",
        "Then the gradient of $z$ is:\n",
        "\n",
        "$$ \\nabla z = \\left ( \\frac{\\partial z}{\\partial x_0},\\frac{\\partial z}{\\partial x_1}, ... \\frac{\\partial z}{\\partial x_{n-1}} \\right ) = \\left ( \\frac{\\partial (\\prod_{i=0}^{n-1} x_i)}{\\partial x_0},\\frac{\\partial (\\prod_{i=0}^{n-1} x_i)}{\\partial x_1}, ... ,\\frac{\\partial (\\prod_{i=0}^{n-1} x_i)}{\\partial x_{n-1}}  \\right )$$\n",
        "\n",
        "Thus,\n",
        "\n",
        "$$ \\frac{\\partial z}{\\partial x_j} = \\frac{\\prod_{i=0}^{n-1} x_i} {x_j} $$\n",
        "\n",
        "Note that the input x now is a list of numbers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLRYCP1gevym"
      },
      "source": [
        "class MultiplyAll:\n",
        "    def forward(self, array):\n",
        "        self.x = np.array(array)\n",
        "        return np.product(self.x)\n",
        "    def backward(self):\n",
        "        return np.array(list(np.prod(self.x) / i for i in self.x))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knVe-PZUSOcn",
        "outputId": "981eecc8-4fbe-4d0d-b8de-7ec566d22c1c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "multimult = MultiplyAll()\n",
        "print(multimult.forward([5.0,2.5,3.0]))\n",
        "print(multimult.backward())"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "37.5\n",
            "[ 7.5 15.  12.5]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPgiEBwDaCNV"
      },
      "source": [
        "# 2. Log Function Node"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JurMNDckaCNV"
      },
      "source": [
        "If $z=f(x)=log(x)$, then the gradient of $z$ is\n",
        "\n",
        "\n",
        "$$\n",
        "\\nabla z = \\left ( \\frac{\\partial z}{\\partial x}\\right ) = \\left ( \\frac{\\partial (log(x))}{\\partial x}\\right ) = \\frac{1}{x}\n",
        "$$\n",
        "\n",
        "<img src=\"http://drive.google.com/uc?export=view&id=1YKou4QvGFwjWV_zzk0H8cMS0vGT7lFMB\" width=\"600\">\n",
        "\n",
        "> **Forward Propagation**\n",
        ">-  Log node returns the log value of the input.\n",
        "\n",
        "> **Back Propagation**\n",
        ">-  Log node returns the inverse value of the input."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDWtIaNcaCNZ"
      },
      "source": [
        "# Define Log class and its forward, back propagation methods.\n",
        "class Log:\n",
        "    \n",
        "    # f(x) = log(x)\n",
        "    def forward(self, x):\n",
        "        self.x = x\n",
        "        return np.log(self.x)\n",
        "        \n",
        "    # df/dx = 1/x\n",
        "    def backward(self):\n",
        "        return 1.0/self.x"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghBa9P5-aCNd"
      },
      "source": [
        "Let's create log node object, set **x=2** and see if forward, backward propagation values are returned correctly as **0.693** and **0.5**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5g7x8qhYaCNe",
        "outputId": "d909df1e-2619-48d6-8a63-947692f4f132",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "log = Log()\n",
        "x = 2\n",
        "log.forward(x), log.backward()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.6931471805599453, 0.5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ltj5Oy5xaCNh"
      },
      "source": [
        "# 3. Square Node"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nz_balW9aCNh"
      },
      "source": [
        "If $z=f(x)=x^2$, then the gradient of $z$ is\n",
        "\n",
        "\n",
        "$$\n",
        "\\nabla z = \\left ( \\frac{\\partial z}{\\partial x}\\right ) = \\left ( \\frac{\\partial x^2}{\\partial x}\\right ) = 2x\n",
        "$$\n",
        "\n",
        "<img src=\"http://drive.google.com/uc?export=view&id=1C67JqOdnW4dxbLBVHLxF8Hvrr2NoTth3\" width=\"600\">\n",
        "\n",
        "\n",
        "> **Forward Propagation**\n",
        ">-  Square node returns the square value of the input.  \n",
        "\n",
        "> **Back Propagation**\n",
        ">-  Square node returns the value of $2x$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NuXbxWmMaCNi"
      },
      "source": [
        "# Define Square class and its forward, back propagation methods.\n",
        "\n",
        "class Square:\n",
        "    \n",
        "    # f(x) = x^2\n",
        "    def forward(self, x):\n",
        "        self.x = x\n",
        "        return self.x ** 2\n",
        "    # df/dx = 2x\n",
        "    def backward(self):\n",
        "        return 2 * self.x"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1pXnktfQaCNl",
        "outputId": "a01e1f30-63e4-4cfe-c1f9-17c01ba8e280",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "square = Square()\n",
        "x =5\n",
        "square.forward(x), square.backward()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25, 10)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azQeSHUkaCNn"
      },
      "source": [
        "#4. Trigonometric Functions(sin, cos, tan)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_drsjry_aCNo"
      },
      "source": [
        "##4-1. Sin Node\n",
        "If $z=f(x)=sin(x)$, then the gradient of $z$ is\n",
        "\n",
        "\n",
        "$$\n",
        "\\nabla z = \\left ( \\frac{\\partial z}{\\partial x}\\right ) = \\left ( \\frac{\\partial sin(x)}{\\partial x}\\right ) = cos(x)\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "<img src=\"http://drive.google.com/uc?export=view&id=1zdwScJP5hbMTtJETFVo1P9F8ZACoU-fc\" width=\"600\">\n",
        "\n",
        "> **Forward Propagation**\n",
        ">-  Sin node returns the sine value of the input.\n",
        "  \n",
        "\n",
        "> **Back Propagation**\n",
        ">-  Sin node returns the cosine value of the input."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqYs08D-aCNo"
      },
      "source": [
        "# Define Sin class and its forward, back propagation methods.\n",
        "class Sin:\n",
        "    \n",
        "    def forward(self, x):\n",
        "        self.x = x\n",
        "        return np.sin(x)\n",
        "\n",
        "    def backward(self):\n",
        "        return np.cos(x)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNRSoqRiaCNu",
        "outputId": "cdf0ef94-514d-4165-8ebb-6fdc03212f37",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "sin = Sin()\n",
        "x = np.pi / 3\n",
        "sin.forward(x), sin.backward()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.8660254037844386, 0.5000000000000001)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYzIZtXMaCNw"
      },
      "source": [
        "##4-2. Cos Node\n",
        "If $z=f(x)=cos(x)$, then the gradient of $z$ is\n",
        "\n",
        "\n",
        "$$\n",
        "\\nabla z = \\left ( \\frac{\\partial z}{\\partial x}\\right ) = \\left ( \\frac{\\partial cos(x)}{\\partial x}\\right ) = -sin(x)\n",
        "$$\n",
        "\n",
        "<img src=\"http://drive.google.com/uc?export=view&id=11AEEdSWOmBjGQhzW2-BDboXFjCvd-hi8\" width=\"600\">\n",
        "\n",
        "\n",
        "> **Forward Propagation**\n",
        ">-  Cos node returns the cosine value of the input.\n",
        "  \n",
        "\n",
        "> **Back Propagation**\n",
        ">-  Cos node returns the -sine value of the input."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "msMjBRfVaCNx"
      },
      "source": [
        "class Cos:\n",
        "    def forward(self, x):\n",
        "        self.x = x\n",
        "        return np.cos(x)\n",
        "    \n",
        "    def backward(self):\n",
        "        return -np.sin(x)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-TAnRiqaCN1",
        "outputId": "ea65cb9b-6c58-49cb-f83a-ba9a93e243dd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "cos = Cos()\n",
        "x = np.pi/3\n",
        "cos.forward(x), cos.backward()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.5000000000000001, -0.8660254037844386)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMYxbVeYaCN4"
      },
      "source": [
        "##4-3. Tan Node\n",
        "If $z=f(x)=tan(x)$, then the gradient of $z$ is\n",
        "\n",
        "\n",
        "$$\n",
        "\\nabla z = \\left ( \\frac{\\partial z}{\\partial x}\\right ) = \\left ( \\frac{\\partial tan(x)}{\\partial x}\\right ) = \\frac{1}{ cos(x)^2}\n",
        "$$\n",
        "\n",
        "<img src=\"http://drive.google.com/uc?export=view&id=16Q1LDQ4L2uY8dkqKgMRZP8UILcKtG10L\" width=\"600\">\n",
        "\n",
        "\n",
        "> **Forward Propagation**\n",
        ">- Tan node returns the tangent value of the input.\n",
        "  \n",
        "\n",
        "> **Back Propagation**\n",
        ">- Tan node returns $\\frac{1}{ cos(x)^2}$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m5uA5dVxaCN4"
      },
      "source": [
        "class Tan:\n",
        "    \n",
        "    def forward(self, x):\n",
        "        self.x = x\n",
        "        return np.tan(x)\n",
        "    \n",
        "    def backward(self):\n",
        "        return 1 / (np.cos(x)**2)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LbJdzx_MaCN7",
        "outputId": "5eb1cf43-a6f2-4119-8374-61de0e1758cb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "tan = Tan()\n",
        "x = np.pi/3\n",
        "tan.forward(x), tan.backward()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1.7320508075688767, 3.9999999999999982)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8ug_8DLaCN-"
      },
      "source": [
        "#5. Loss Function\n",
        "Let's define the loss funtions through computational graph and get the partial derivative of the loss funcions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZxeAqFndaCN_"
      },
      "source": [
        "##5-1. MSE Loss Function Node"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-A2vqK8aCN_"
      },
      "source": [
        "MSE(Mean Squared Error) Loss Function can be represented as following.\n",
        "\n",
        "$$\n",
        "MSE=\\frac{1}{2} (\\hat{y}-y)^2\n",
        "$$\n",
        "\n",
        "\n",
        "n represents the total number of data, $y^{(i)}$ represents label of i'th data.\n",
        "\n",
        " predicted value of ${y}^{(i)}$: $\\hat{y}^{(i)}=\\sigma(w^Tx^{(i)}+b)$\n",
        " and Cost Function of MSE can be represented as\n",
        "\n",
        "\n",
        "$$\n",
        "MSE(Cost)=\\frac{1}{n}\\sum_{i=1}^{n}\\frac{1}{2} (\\hat{y}^{(i)}-y^{(i)})^2\n",
        "$$\n",
        "\n",
        "Cost Function is just an average of loss function, so I'll only define Loss Function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQePtNbtaCOA"
      },
      "source": [
        "Below shows the Computatioanl Graph of the MSE Loss Function above.\n",
        "\n",
        "<img src=\"http://drive.google.com/uc?export=view&id=1dqfw6Tpeo6CkNsns8IMU3Bw1ItkAUNZ6\" width=\"600\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1X7zKIfaCOB"
      },
      "source": [
        "To find the optimal weights $w$ and bias $b$ for the Gradient Descent algorithm, we need the partial derivative values of the Loss by $w$ and $b$.  Thus, our MSE node should return the partial derivative value of $\\hat{y}$, $\\frac{\\partial L}{\\partial \\hat{y}} $\n",
        "\n",
        "MSE Node can be represented as following diagram.\n",
        "\n",
        "<img src=\"http://drive.google.com/uc?export=view&id=16dbnhIahsvpVh1eAF4uavMA8mZIMkWhF\" width=\"600\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-A8ues2cITX"
      },
      "source": [
        "To define MSE Node, we'll first define Add Node.\n",
        "Forward will return the sum of two inputs, and backward will return 1 to each path."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3vpAey6aCOC"
      },
      "source": [
        "class Add:\n",
        "    def forward(self, x, y):\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "        return x + y\n",
        "    def backward(self):\n",
        "        return 1.0, 1.0"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWbTSEoLaCOB"
      },
      "source": [
        "Let's define MSE Node.\n",
        "\n",
        "When $\\hat{y}$ and $y$ are given as inputs, forward propagation will return $ MSE=\\frac{1}{2} (\\hat{y}-y)^2 $ and backward propagation will return  $\\frac{\\partial L}{\\partial \\hat{y}} $."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MfU74gvUaCOE"
      },
      "source": [
        "class MSE:\n",
        "    mult1 = Multiply()\n",
        "    mult2 = Multiply()\n",
        "    add = Add()\n",
        "    sq = Square()\n",
        "    def forward(self, y_predict, y_actual):\n",
        "        self.y_predict = y_predict\n",
        "        self.y_actual = y_actual\n",
        "        fw1 = self.mult1.forward(self.y_actual, -1)\n",
        "        fw2 = self.add.forward(fw1, self.y_predict)\n",
        "        fw3 = self.sq.forward(fw2)\n",
        "        fw4 = self.mult2.forward(fw3, 1/2)\n",
        "        return fw4\n",
        "    \n",
        "    def backward(self):\n",
        "        bw1 = self.mult2.backward()[0]\n",
        "        bw2 = self.sq.backward() * bw1\n",
        "        bw3 = self.add.backward()[1] * bw2\n",
        "        return bw3"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPsYHdqkaCOG"
      },
      "source": [
        "Create MSE node object, set **y_predict = 1, y_actual = 4** and check if the forward, backward values are **4.5, -3.0**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7w-M2YJ3aCOG",
        "outputId": "19c4466f-0c68-4cf8-858a-162f67d03a27",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "mse = MSE()\n",
        "mse.forward(1,4), mse.backward()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4.5, -3.0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QM34LQNpaCOI"
      },
      "source": [
        "## 5-2. Cross Entropy Loss Function Node"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJcPPAquaCOJ"
      },
      "source": [
        "Cross entropy node can be represented as: \n",
        "\n",
        "<img src=\"http://drive.google.com/uc?export=view&id=15wLtFHEr884R75PZrl7TLTUj2izJkWM-\" width=\"600\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWJ5eRGPaCOJ"
      },
      "source": [
        "###Cross Entropy\n",
        "If total number of class(label) is C, $y_{c}$ represents c'th label value, predicted value of ${y}_{c}$ : $\\hat{y}_{c}=\\sigma(w^Tx_{c}+b)$,\n",
        "\n",
        "then: \n",
        "\n",
        "$$\n",
        "\\text{Cross Entropy Loss} = -\\sum_{c=1}^{C}  y_{c} \\times log(\\hat{y}_{c})\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGZkIVEGbfhk"
      },
      "source": [
        "###Binary Classification\n",
        "For Binary Classification problem, $C = 2$, thus:\n",
        "\n",
        "$$ \\text{Cross Entropy Loss(binary)} = - y \\times log(\\hat{y}) - (1-{y}) \\times log(1-\\hat{y})$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_T8oWXQaCOJ"
      },
      "source": [
        "### 3 Class Classification\n",
        "For 3 Class Multi-class classification, $C = 3$, thus:\n",
        "\n",
        "$$\n",
        "\\text{Cross Entropy Loss} = -\\sum_{c=1}^{3}  y_{c} \\times log(\\hat{y}_{c})\n",
        "$$\n",
        "\n",
        "3 Class Multi-class classification Cross Entropy Node can be represented as computational graph as below. \n",
        "\n",
        "<img src=\"http://drive.google.com/uc?export=view&id=1HFLacOYnnN5ihTN9pm1_LEL1CB-1JNny\" width=\"600\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfg42r1haCOK"
      },
      "source": [
        "As before, let's start by defining Add function for multiple inputs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XndOZj7UaCOQ"
      },
      "source": [
        "class AddAll():\n",
        "    def forward(self, array):\n",
        "        self.array = np.array(array)\n",
        "        return self.array.sum()\n",
        "    \n",
        "    def backward(self):\n",
        "        return np.array([1 for i in range (self.array.shape[0])])"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDQ-rvDQaCOS"
      },
      "source": [
        "class CE:\n",
        "    log = Log()\n",
        "    mult = Multiply()\n",
        "    addAll = AddAll()\n",
        "    lastMult = Multiply()\n",
        "    def forward(self, y_predict, y_actual):\n",
        "        self.y_predict = np.array(y_predict)\n",
        "        self.y_actual = np.array(y_actual)\n",
        "        log_y_predict = self.log.forward(self.y_predict)\n",
        "        log_yp_times_y = self.mult.forward(log_y_predict, self.y_actual)\n",
        "        added = self.addAll.forward(log_yp_times_y)\n",
        "        multiplied = self.lastMult.forward(added, -1)\n",
        "        return multiplied\n",
        "    \n",
        "    def backward(self):\n",
        "        bw1 = self.lastMult.backward()[0]\n",
        "        bw2 = bw1 * self.addAll.backward()\n",
        "        bw3 = bw2 * self.mult.backward()[0]\n",
        "        bw4 = bw3 * self.log.backward()\n",
        "        return bw4"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPw6lgpUaCOV"
      },
      "source": [
        "Build a CE Node object, set **y_predict_ce = [2, 3, 4] , y_ce = [0, 1, 2]** and see if the forward and backward value show the correct values: **-3.8712 and (0, -0.33, -0.5)**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FlkkAQYbaCOV",
        "outputId": "19826699-dd73-4f00-9f26-4c6e366a66ed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "ce = CE()\n",
        "y_predict_ce = [2, 3, 4]\n",
        "y_ce = [0, 1, 2]\n",
        "print(f'Cross Entropy: {ce.forward(y_predict_ce, y_ce)}\\nBackward Propagation: {ce.backward()}')"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cross Entropy: -3.8712010109078907\n",
            "Backward Propagation: [ 0.         -0.33333333 -0.5       ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxPZ6sEXiL6T"
      },
      "source": [
        ""
      ],
      "execution_count": 27,
      "outputs": []
    }
  ]
}