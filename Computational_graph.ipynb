{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Computational-graph.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arkincognito/ML-from-scrap/blob/master/Computational_graph.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tv99OtnSbdpn"
      },
      "source": [
        "## This notebook is based on an assignment from DS School's Deep Learning Course."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37k0KiDRaCND"
      },
      "source": [
        "Computational Graph enables Partial Derivation without explicitly representing the partial derivative.\n",
        "\n",
        "In this notebook, I'll write computational graph code for\n",
        "\n",
        "*Nodes(Log, Square, Trigonometric Functions)\n",
        "and\n",
        "*Loss Functions(Mean Square Error, Cross Entropy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6hbNz8baCNE"
      },
      "source": [
        "## Computational Graph 복습"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfn02vgDaCNE"
      },
      "source": [
        "computational graph는 계산과정을 그래프로 나타난 것으로, 노드는 연산을 엣지는 입출력 관계를 의미합니다. computational graph를 통해 아무리 복잡한 계산도 각 노드별로의 local gradient만 계산하기 때문에 효율적으로 계산이 가능합니다. 각 노드는 자신과 관련된 계산 외에는 고려하지 않고 중간 미분 결과를 공유하여 다수의 미분을 효율적으로 계산합니다. chain rule의 과정을 시각적으로 표현한 것이라고도 볼 수 있습니다. \n",
        "\n",
        "<img src=\"http://drive.google.com/uc?export=view&id=14x4zQpEEatgMb1W0BY47lXKjM5haZq1x\" width=\"600\">\n",
        "\n",
        "\n",
        ">- **Forward Propagation**\n",
        "\n",
        ">  **순전파(forward propagation)**는 뉴럴 네트워크의 입력층부터 출력층까지 순서대로 변수들을 계산하고 저장합니다. 그래프 상에서 왼쪽에서 오른쪽으로 진행되는 파란색으로 표시된 계산입니다.\n",
        "  \n",
        "\n",
        ">- **Back Propagation**\n",
        "\n",
        ">  **역전파(back propagaton)**는 네트워크 전체에 대해 반복적으로 chain rule을 적용하여 gradient를 계산하여 뉴럴 네트워크를 효율적으로 학습하는 데에 사용되는 알고리즘입니다. 오차를 역 방향으로 전파하는 방법이라 오차역전파법이라고도 말합니다. 중간 변수와 가중치에 대한 gradient를 계산하고 저장합니다. 그래프 상에서 오른쪽에서 왼쪽으로 진행되는 빨간색으로 표시된 계산입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7HBPc9BaCNF"
      },
      "source": [
        "#### Multiply 노드의 Forward/Back Propagation를 구하는 Class를 아래와 같이 만들어 보았습니다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RuxtY3XYaCNF"
      },
      "source": [
        "- #### Multiply 노드\n",
        "\n",
        "곱셈 노드의 수식이 $z=f(x,y)=x\\times y $일 때 $z$의 gradient는 아래와 같습니다. \n",
        "\n",
        "$$\n",
        "\\nabla z = \\left ( \\frac{\\partial z}{\\partial x},\\frac{\\partial z}{\\partial y}  \\right ) = \\left ( \\frac{\\partial (xy)}{\\partial x},\\frac{\\partial (xy)}{\\partial y}  \\right ) = (y,x)\n",
        "$$\n",
        "\n",
        "<img src=\"http://drive.google.com/uc?export=view&id=1pQ9HFmr9_31daD8YIhO72IQtr5Kvj2GP\" width=\"600\">\n",
        "\n",
        "\n",
        ">- **Forward Propagation**\n",
        "\n",
        ">  곱셈 노드는 입력된 두 가지 값을 곱하여 다음 노드로 전달합니다.\n",
        "  \n",
        "\n",
        ">- **Back Propagation**\n",
        "\n",
        ">  곱셈 노드는 입력된 값에 forward propagation 당시의 입력 변수들을 서로 바꾸어 곱하여 다음 노드로 전달합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0uqQOY1aCNG"
      },
      "source": [
        "# 곱셈 노드에 적용되는\n",
        "# forward, back propagation 메소드를 정의합니다.\n",
        "class Multiply:\n",
        "    # f(x,y) = xy\n",
        "    def forward(self, x, y):\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "        # 두 가지 값을 곱하여 다음 노드로 전달합니다.\n",
        "        return self.x * self.y\n",
        "    # dfdx = y, dfdy = x\n",
        "    def backward(self):\n",
        "        dx = self.y\n",
        "        dy = self.x\n",
        "        # forward propagation의 입력 변수들을\n",
        "        # 서로 바꾸어 곱하여 다음 노드로 전달합니다.\n",
        "        return dx, dy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34gLNovEaCNK",
        "outputId": "44eca312-6af7-47df-be23-d3c6f5185e7f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "multiply=Multiply()\n",
        "forward2 = multiply.forward(10,3)\n",
        "forward2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J52qgUt1aCNR",
        "outputId": "deb03b7a-172c-4bb9-f5b2-c7bbff3c6ec1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "multiply.backward()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3, 10)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQUczXl1cfmp"
      },
      "source": [
        "More generally, multiplication can be written as $z = f(x_0, x_1, ... , x_{n-1}) = $$\\prod_{i=0}^{n-1} x_i$.\n",
        "\n",
        "Then the gradient of $z$ is:\n",
        "\n",
        "$$ \\nabla z = \\left ( \\frac{\\partial z}{\\partial x_0},\\frac{\\partial z}{\\partial x_1}, ... \\frac{\\partial z}{\\partial x_{n-1}} \\right ) = \\left ( \\frac{\\partial (\\prod_{i=0}^{n-1} x_i)}{\\partial x_0},\\frac{\\partial (\\prod_{i=0}^{n-1} x_i)}{\\partial x_1}, ... ,\\frac{\\partial (\\prod_{i=0}^{n-1} x_i)}{\\partial x_{n-1}}  \\right )$$\n",
        "\n",
        "Thus,\n",
        "\n",
        "$$ \\frac{\\partial z}{\\partial x_j} = \\frac{\\prod_{i=0}^{n-1} x_i} {x_j} $$\n",
        "\n",
        "Note that the input x now is the list of numbers we're solving for product."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLRYCP1gevym"
      },
      "source": [
        "class MultiplyAll:\n",
        "    def forward(self, array):\n",
        "        self.x = np.array(array)\n",
        "        return np.product(self.x)\n",
        "    def backward(self):\n",
        "        return np.array(list(np.prod(self.x) / i for i in self.x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPgiEBwDaCNV"
      },
      "source": [
        "### 1. 로그함수 노드 구현해보기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JurMNDckaCNV"
      },
      "source": [
        "자연대수 $e$를 밑으로 하는 로그함수 노드의 수식이 $z=f(x)=log(x)$일 때 $z$의 gradient는 아래와 같습니다.\n",
        "\n",
        "\n",
        "$$\n",
        "\\nabla z = \\left ( \\frac{\\partial z}{\\partial x}\\right ) = \\left ( \\frac{\\partial (log(x))}{\\partial x}\\right ) = \\frac{1}{x}\n",
        "$$\n",
        "\n",
        "<img src=\"http://drive.google.com/uc?export=view&id=1YKou4QvGFwjWV_zzk0H8cMS0vGT7lFMB\" width=\"600\">\n",
        "\n",
        ">- **Forward Propagation**\n",
        "\n",
        ">  로그 노드는 입력된 값을 밑이 $e$인 로그함수의 진수로 취하여 다음 노드로 전달합니다.\n",
        "  \n",
        "\n",
        ">- **Back Propagation**\n",
        "\n",
        ">  로그 노드는 입력된 값에 역수를 취한 값을 곱하여 다음 노드로 전달합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCF01NXtaCNW"
      },
      "source": [
        "# 로그를 계산할 수 있는 numpy 패키지를 가져옵니다. np라는 축약어로 사용합니다. \n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDWtIaNcaCNZ"
      },
      "source": [
        "# 로그 노드에 적용되는\n",
        "# forward, back propagation 메소드를 정의합니다.\n",
        "class Log:\n",
        "    \n",
        "    # f(x) = log(x)\n",
        "    def forward(self, x):\n",
        "        self.x = x\n",
        "        return np.log(self.x)\n",
        "        \n",
        "    # dfdx = 1/x\n",
        "    def backward(self):\n",
        "        return 1.0/self.x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghBa9P5-aCNd"
      },
      "source": [
        "log 노드 객체를 생성하고, **x=2**값을 넣어준 후 forward, backward값이 각각 **0.693, 0.5** 으로 잘 구해지는지 확인해 봅시다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5g7x8qhYaCNe",
        "outputId": "15da49ee-6a87-4c7d-81c3-0d9adb5bb7f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "log = Log()\n",
        "x = 2\n",
        "log.forward(x), log.backward()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.6931471805599453, 0.5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ltj5Oy5xaCNh"
      },
      "source": [
        "### 2. 제곱(Square) 노드 구현해보기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nz_balW9aCNh"
      },
      "source": [
        "제곱 노드의 수식이 $z=f(x)=x^2$일 때 $z$의 gradient는 아래와 같습니다.\n",
        "\n",
        "\n",
        "$$\n",
        "\\nabla z = \\left ( \\frac{\\partial z}{\\partial x}\\right ) = \\left ( \\frac{\\partial x^2}{\\partial x}\\right ) = 2x\n",
        "$$\n",
        "\n",
        "<img src=\"http://drive.google.com/uc?export=view&id=1C67JqOdnW4dxbLBVHLxF8Hvrr2NoTth3\" width=\"600\">\n",
        "\n",
        "\n",
        ">- **Forward Propagation**\n",
        "\n",
        ">  로그 노드는 입력된 값에 제곱을 취하여 다음 노드로 전달합니다.\n",
        "  \n",
        "\n",
        ">- **Back Propagation**\n",
        "\n",
        ">  로그 노드는 입력된 값에 $2x$를 곱하여 다음 노드로 전달합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NuXbxWmMaCNi"
      },
      "source": [
        "# 제곱 노드에 적용되는\n",
        "# forward, back propagation 메소드를 정의합니다.\n",
        "class Square:\n",
        "    \n",
        "    # f(x) = x^2\n",
        "    def forward(self, x):\n",
        "        self.x = x\n",
        "        return self.x ** 2\n",
        "    # dfdx = 2x\n",
        "    def backward(self):\n",
        "        return 2 * self.x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgxK2dhqaCNk"
      },
      "source": [
        "square 노드 객체를 생성하고, **x=5**값을 넣어준 후 forward, backward값이 각각 **25, 10** 으로 잘 구해지는지 확인해 봅시다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1pXnktfQaCNl",
        "outputId": "78e0ed09-552c-4ffe-d810-9dd518629514",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "square = Square()\n",
        "x =5\n",
        "square.forward(x), square.backward()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25, 10)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azQeSHUkaCNn"
      },
      "source": [
        "### 3. 삼각함수(sin, cos, tan) 노드 구현해보기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_drsjry_aCNo"
      },
      "source": [
        "#### 3-1. sin함수 노드 구현해보기\n",
        "sin함수 노드의 수식이 $z=f(x)=sin(x)$일 때 $z$의 gradient는 아래와 같습니다.\n",
        "\n",
        "\n",
        "$$\n",
        "\\nabla z = \\left ( \\frac{\\partial z}{\\partial x}\\right ) = \\left ( \\frac{\\partial sin(x)}{\\partial x}\\right ) = cos(x)\n",
        "$$\n",
        "<img src=\"http://drive.google.com/uc?export=view&id=1zdwScJP5hbMTtJETFVo1P9F8ZACoU-fc\" width=\"600\">\n",
        "\n",
        ">- **Forward Propagation**\n",
        "\n",
        ">  sin 노드는 입력된 값에 sin함수를 취하여 다음 노드로 전달합니다.\n",
        "  \n",
        "\n",
        ">- **Back Propagation**\n",
        "\n",
        ">  sin 노드는 입력된 값에 $cos(x)$를 곱하여 다음 노드로 전달합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqYs08D-aCNo"
      },
      "source": [
        "# sin 함수 노드에 적용되는\n",
        "# forward, back propagation 메소드를 정의합니다.\n",
        "class Sin:\n",
        "    \n",
        "    def forward(self, x):\n",
        "        self.x = x\n",
        "        return np.sin(x)\n",
        "\n",
        "    def backward(self):\n",
        "        return np.cos(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLYifvGzaCNt"
      },
      "source": [
        "sin 노드 객체를 생성하고, **x=60&deg;**값을 넣어준 후 forward, backward값이 각각 **0.866, 0.5** 으로 잘 구해지는지 확인해 봅시다.\n",
        "(numpy의 pi를 사용하여 x=np.pi / 3 로 넣어 줄 수 있습니다.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNRSoqRiaCNu",
        "outputId": "081b4806-390d-40a6-e9fc-464c4610230a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "sin = Sin()\n",
        "x = np.pi / 3\n",
        "sin.forward(x), sin.backward()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.8660254037844386, 0.5000000000000001)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYzIZtXMaCNw"
      },
      "source": [
        "#### 3-2. cos함수 노드 구현해보기\n",
        "cos함수 노드의 수식이 $z=f(x)=cos(x)$일 때 $z$의 gradient는 아래와 같습니다.\n",
        "\n",
        "\n",
        "$$\n",
        "\\nabla z = \\left ( \\frac{\\partial z}{\\partial x}\\right ) = \\left ( \\frac{\\partial cos(x)}{\\partial x}\\right ) = -sin(x)\n",
        "$$\n",
        "\n",
        "<img src=\"http://drive.google.com/uc?export=view&id=11AEEdSWOmBjGQhzW2-BDboXFjCvd-hi8\" width=\"600\">\n",
        "\n",
        "\n",
        ">- **Forward Propagation**\n",
        "\n",
        ">  cos 노드는 입력된 값에 cos함수를 취하여 다음 노드로 전달합니다.\n",
        "  \n",
        "\n",
        ">- **Back Propagation**\n",
        "\n",
        ">  cos 노드는 입력된 값에 $-sin(x)$를 곱하여 다음 노드로 전달합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "msMjBRfVaCNx"
      },
      "source": [
        "class Cos:\n",
        "    \n",
        "    def forward(self, x):\n",
        "        self.x = x\n",
        "        return np.cos(x)\n",
        "    \n",
        "    def backward(self):\n",
        "        return -np.sin(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpD5XmZTaCN0"
      },
      "source": [
        "cos 노드 객체를 생성하고, **x=60&deg;**값을 넣어준 후 forward, backward값이 각각 **0.5, -0.866** 으로 잘 구해지는지 확인해 봅시다.\n",
        "(numpy의 pi를 사용하여 x=np.pi / 3 로 넣어 줄 수 있습니다.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-TAnRiqaCN1",
        "outputId": "c6b61970-9fbe-4a79-9394-b6abd67c9d2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cos = Cos()\n",
        "x = np.pi/3\n",
        "cos.forward(x), cos.backward()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.5000000000000001, -0.8660254037844386)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMYxbVeYaCN4"
      },
      "source": [
        "#### 3-3. tan함수 노드 구현해보기\n",
        "tan함수 노드의 수식이 $z=f(x)=tan(x)$일 때 $z$의 gradient는 아래와 같습니다.\n",
        "\n",
        "\n",
        "$$\n",
        "\\nabla z = \\left ( \\frac{\\partial z}{\\partial x}\\right ) = \\left ( \\frac{\\partial tan(x)}{\\partial x}\\right ) = \\frac{1}{ cos(x)^2}\n",
        "$$\n",
        "\n",
        "<img src=\"http://drive.google.com/uc?export=view&id=16Q1LDQ4L2uY8dkqKgMRZP8UILcKtG10L\" width=\"600\">\n",
        "\n",
        "\n",
        ">- **Forward Propagation**\n",
        "\n",
        ">  tan 노드는 입력된 값에 tan함수를 취하여 다음 노드로 전달합니다.\n",
        "  \n",
        "\n",
        ">- **Back Propagation**\n",
        "\n",
        ">  tan 노드는 입력된 값에 $\\frac{1}{ cos(x)^2}$를 곱하여 다음 노드로 전달합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m5uA5dVxaCN4"
      },
      "source": [
        "class Tan:\n",
        "    \n",
        "    def forward(self, x):\n",
        "        self.x = x\n",
        "        return np.tan(x)\n",
        "    \n",
        "    def backward(self):\n",
        "        return 1 / (np.cos(x)**2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EH_RlUb_aCN7"
      },
      "source": [
        "cos 노드 객체를 생성하고, **x=60&deg;**값을 넣어준 후 forward, backward값이 각각 **1.732, 4** 으로 잘 구해지는지 확인해 봅시다.\n",
        "(numpy의 pi를 사용하여 x=np.pi / 3 로 넣어 줄 수 있습니다.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LbJdzx_MaCN7",
        "outputId": "d7595202-1f9a-45eb-8e09-49ed68932a2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tan = Tan()\n",
        "x = np.pi/3\n",
        "tan.forward(x), tan.backward()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1.7320508075688767, 3.9999999999999982)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8ug_8DLaCN-"
      },
      "source": [
        "#### 이제, 그동안 다뤄봤었던 Loss Function을 Computational Graph로 그리고 이를 이용하여 Loss Function의 편미분을 구현해 보겠습니다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZxeAqFndaCN_"
      },
      "source": [
        "### 4. MSE Loss Function 노드 구현해보기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-A2vqK8aCN_"
      },
      "source": [
        "Regression 모델에서 주로 사용하는 평균제곱오차(MSE; Mean Squared Error) Loss Function (데이터 한 개의 Error)은 다음과 같이 나타낼 수 있습니다. (여기서는  $\\frac{1}{2}$을 추가해주었는데 경우에 따라 생략하기도 합니다.)\n",
        "\n",
        "$$\n",
        "MSE=\\frac{1}{2} (\\hat{y}-y)^2\n",
        "$$\n",
        "\n",
        "\n",
        "데이터의 총 갯수를 n개라 하고, $y^{(i)}$는 i번째 데이터의 label값, $\\hat{y}^{(i)}$은 ${y}^{(i)}$의 예측치로 $\\hat{y}^{(i)}=\\sigma(w^Tx^{(i)}+b)$라 하고 MSE의 Cost Function(비용함수:모든 Loss의 평균)를 나타내면 다음과 같습니다. \n",
        "\n",
        "\n",
        "$$\n",
        "MSE(Cost)=\\frac{1}{n}\\sum_{i=1}^{n}\\frac{1}{2} (\\hat{y}^{(i)}-y^{(i)})^2\n",
        "$$\n",
        "\n",
        "Cost Function은 Loss Function의 단순 평균이므로, 여기서는 Loss Function만 구현해 보도록 하겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQePtNbtaCOA"
      },
      "source": [
        "위의 MSE Loss Function을 Computatioanl Graph로 그리면 다음과 같습니다. \n",
        "\n",
        "<img src=\"http://drive.google.com/uc?export=view&id=1dqfw6Tpeo6CkNsns8IMU3Bw1ItkAUNZ6\" width=\"600\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1X7zKIfaCOB"
      },
      "source": [
        "이 Loss Function을 Gradient Descent 알고리즘에 적용하여 최적의 w와 b를 구해주기 위해서는 Loss 값을 w와 b로 편미분한 값을 필요로 했습니다. 따라서, 우리는 위 MSE Computational Graph에서 예측치인 $\\hat{y}$에 대한 Loss의 편미분 값. $\\frac{\\partial L}{\\partial \\hat{y}} $ 을 구해주어야 합니다. \n",
        "즉, 지금 우리가 만들어주려하는 MSE 노드는 다음과 같이 나타낼 수 있습니다. \n",
        "\n",
        "<img src=\"http://drive.google.com/uc?export=view&id=16dbnhIahsvpVh1eAF4uavMA8mZIMkWhF\" width=\"600\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWbTSEoLaCOB"
      },
      "source": [
        "이번 4번 과제는 위와 같이 MSE 노드에 $\\hat{y}$과 $y$를 입력받았을 때, forward propagation으로 $ MSE=\\frac{1}{2} (\\hat{y}-y)^2 $ 을 출력하고, backward propagation으로는 $\\frac{\\partial L}{\\partial \\hat{y}} $ 값을 return 해주는 MSE 노드를 만드는 것입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3vpAey6aCOC"
      },
      "source": [
        "class Add:\n",
        "    def forward(self, x, y):\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "        return x + y\n",
        "    def backward(self):\n",
        "        return 1.0, 1.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MfU74gvUaCOE"
      },
      "source": [
        "class MSE:\n",
        "    mult1 = Multiply()\n",
        "    mult2 = Multiply()\n",
        "    add = Add()\n",
        "    sq = Square()\n",
        "    def forward(self, y_predict, y):\n",
        "        self.y_predict = y_predict\n",
        "        self.y = y\n",
        "        fw1 = self.mult1.forward(self.y, -1)\n",
        "        fw2 = self.add.forward(fw1, self.y_predict)\n",
        "        fw3 = self.sq.forward(fw2)\n",
        "        fw4 = self.mult2.forward(fw3, 1/2)\n",
        "        return fw4\n",
        "    \n",
        "    def backward(self):\n",
        "        bw1 = self.mult2.backward()[0]\n",
        "        bw2 = self.sq.backward() * bw1\n",
        "        bw3 = self.add.backward()[1] * bw2\n",
        "        return bw3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPsYHdqkaCOG"
      },
      "source": [
        "MSE 노드 객체를 생성하고, **y_predict = 1, y = 4**값을 넣어준 후 forward, backward값이 각각 **4.5, -3.0** 으로 잘 구해지는지 확인해 봅시다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7w-M2YJ3aCOG",
        "outputId": "2e1ae3c9-4909-401c-b715-066f04652ddf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "mse = MSE()\n",
        "mse.forward(1,4), mse.backward()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4.5, -3.0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QM34LQNpaCOI"
      },
      "source": [
        "### 5. Cross Entropy Loss Function 노드 구현해보기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWJ5eRGPaCOJ"
      },
      "source": [
        "Cross Entropy Loss Function은 다음과 같이 나타낼 수 있습니다.\n",
        "\n",
        "class(label)의 총 갯수를 C개라 하고, $y_{c}$는 c 번째 label 값, $\\hat{y}_{c}$은 ${y}_{c}$의 예측치로 $\\hat{y}_{c}=\\sigma(w^Tx_{c}+b)$라 하고 Cross Entropy Loss를 나타내면 다음과 같습니다. \n",
        "\n",
        "$$\n",
        "\\text{Cross Entropy Loss} = -\\sum_{c=1}^{C}  y_{c} \\times log(\\hat{y}_{c})\n",
        "$$\n",
        "\n",
        "Binary Classification에서는 C값이 2이므로 다음과 같이 나타낼 수 있습니다.\n",
        "\n",
        "(C값이 1이라고 생각하실 수 있지만, Cross Entropy Loss를 적용해줄 때는 원핫인코딩된 y를 사용하므로 Yes / No 두 개의 label이 생성됩니다. 그리고, $y_{1}+y_{2}=1$ 이므로 $y_{1}$을 그냥 $y$로 나타내면 아래와 같이 익숙한 Loss Function이 나옵니다. )\n",
        "\n",
        "$$ \\text{Cross Entropy Loss(binary)} = - y \\times log(\\hat{y}) - (1-{y}) \\times log(1-\\hat{y})$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_T8oWXQaCOJ"
      },
      "source": [
        "이번 과제에서는 Binary Classification이 아닌 Multi-Class, 즉, C=3인 Multi-Class Classification 의 Cross Entropy Loss를 Computational Graph로 그리고, 이 Cross Entropy Loss의 편미분을 구현해보도록 하겠습니다. \n",
        "\n",
        "C=3를 대입한 Cross Entropy Loss는 다음과 같고, \n",
        "\n",
        "$$\n",
        "\\text{Cross Entropy Loss} = -\\sum_{c=1}^{3}  y_{c} \\times log(\\hat{y}_{c})\n",
        "$$\n",
        "\n",
        "이를 Computational Graph로 그려주면 다음과 같습니다. \n",
        "\n",
        "<img src=\"http://drive.google.com/uc?export=view&id=1HFLacOYnnN5ihTN9pm1_LEL1CB-1JNny\" width=\"600\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJcPPAquaCOJ"
      },
      "source": [
        "이 Loss Function을 Gradient Descent 알고리즘에 적용하여 최적의 w와 b를 구해주기 위해서는 Loss 값을 w와 b로 편미분한 값을 필요로 했습니다. 따라서, 우리는 위 Cross Entropy의 Computational Graph에서 예측치인 $\\hat{y}$에 대한 Loss의 편미분 값. $\\frac{\\partial L}{\\partial \\hat{y}} $ 을 구해주어야 합니다. \n",
        "즉, 지금 우리가 만들어주려하는 CE 노드는 다음과 같이 나타낼 수 있습니다. \n",
        "\n",
        "<img src=\"http://drive.google.com/uc?export=view&id=15wLtFHEr884R75PZrl7TLTUj2izJkWM-\" width=\"600\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfg42r1haCOK"
      },
      "source": [
        "이번 5번 과제는 위와 같이 CE(Cross Entropy) 노드에 $\\hat{y}$과 $y$를 입력받았을 때, forward propagation으로 $ \\text{Cross Entropy Loss} = -\\sum_{c=1}^{3}  y_{c} \\times log(\\hat{y}_{c})$ 을 출력하고, backward propagation으로는 $\\frac{\\partial L}{\\partial \\hat{y}} $ 값을 return 해주는 CE 노드를 만드는 것입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qo9dJQ8CaCOK",
        "outputId": "5abb71fc-389b-4131-9828-5881c194c301",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "mult = Multiply()\n",
        "a = np.array([2,3,4])\n",
        "b = np.array([0,1,2])\n",
        "mult.forward(a,b), mult.backward()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([0, 3, 8]), (array([0, 1, 2]), array([2, 3, 4])))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ly0_xz-iaCON",
        "outputId": "ad19df7b-03c5-4c24-ebd5-9c76ca126d99",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "log = Log()\n",
        "a = np.array([1,2,10])\n",
        "log.forward(a), log.backward()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([0.        , 0.69314718, 2.30258509]), array([1. , 0.5, 0.1]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XndOZj7UaCOQ"
      },
      "source": [
        "class AddAll():\n",
        "    def forward(self, array):\n",
        "        self.array = np.array(array)\n",
        "        return self.array.sum()\n",
        "    \n",
        "    def backward(self):\n",
        "        return np.array([1 for i in range (self.array.shape[0])])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDQ-rvDQaCOS"
      },
      "source": [
        "# CE 노드에 적용되는\n",
        "# forward, back propagation 메소드를 정의합니다.\n",
        "class CE:\n",
        "    log = Log()\n",
        "    mult = Multiply()\n",
        "    addAll = AddAll()\n",
        "    lastMult = Multiply()\n",
        "    def forward(self, y_predict_ce, y_ce):\n",
        "        self.y_predict_ce = np.array(y_predict_ce)\n",
        "        self.y_ce = np.array(y_ce)\n",
        "        log_y_predict = self.log.forward(self.y_predict_ce)\n",
        "        log_yp_times_y = self.mult.forward(log_y_predict, self.y_ce)\n",
        "        added = self.addAll.forward(log_yp_times_y)\n",
        "        multiplied = self.lastMult.forward(added, -1)\n",
        "        return multiplied\n",
        "    \n",
        "    def backward(self):\n",
        "        bw1 = self.lastMult.backward()[0]\n",
        "        bw2 = bw1 * self.addAll.backward()\n",
        "        bw3 = bw2 * self.mult.backward()[0]\n",
        "        bw4 = bw3 * self.log.backward()\n",
        "        return bw4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPw6lgpUaCOV"
      },
      "source": [
        "CE 노드 객체를 생성하고, **y_predict_ce = [2, 3, 4] , y_ce = [0, 1, 2]** 값을 넣어준 후 forward, backward값이 각각 **-3.8712, (0, -0.33, -0.5)** 으로 잘 구해지는지 확인해 봅시다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FlkkAQYbaCOV",
        "outputId": "05031690-d595-4866-c042-77ddb6004e5b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "ce = CE()\n",
        "y_predict_ce = [2, 3, 4]\n",
        "y_ce = [0, 1, 2]\n",
        "print(f'Cross Entropy: {ce.forward(y_predict_ce, y_ce)}\\nBackward Propagation: {ce.backward()}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cross Entropy: -3.8712010109078907\n",
            "Backward Propagation: [ 0.         -0.33333333 -0.5       ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxPZ6sEXiL6T"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}